{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train/Validation/Test Splits (Standalone)\n",
    "\n",
    "This notebook creates stratified train/validation/test splits from the real dataset index.\n",
    "**This is a standalone version that includes all necessary code and doesn't require external files.**\n",
    "\n",
    "## Purpose\n",
    "- Load real dataset index or create dummy data\n",
    "- Perform stratified splitting by wave type and direction\n",
    "- Ensure balanced representation across splits\n",
    "- Analyze split statistics and validate quality\n",
    "- Generate comprehensive visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not available\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "required_packages = ['pandas', 'matplotlib', 'seaborn', 'numpy', 'scikit-learn']\n",
    "for package in required_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "print(\"âœ“ All required packages are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    if path:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read JSONL file and return list of records.\"\"\"\n",
    "    items = []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        items.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Warning: Invalid JSON on line {line_num}: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {path}\")\n",
    "    return items\n",
    "\n",
    "\n",
    "def write_jsonl(items: List[Dict[str, Any]], path: str) -> None:\n",
    "    \"\"\"Write list of records to JSONL file.\"\"\"\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in items:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"âœ“ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - modify these paths as needed\n",
    "CONFIG = {\n",
    "    \"real_index\": \"data/processed/real_index.jsonl\",\n",
    "    \"out_dir\": \"data/processed/splits\",\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,  # Remaining after train and val\n",
    "    \"seed\": 42,\n",
    "    \"create_dummy_if_missing\": True,\n",
    "    \"dummy_samples\": 150\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nSplit ratios - Train: {CONFIG['train_ratio']}, Val: {CONFIG['val_ratio']}, Test: {CONFIG['test_ratio']}\")\n",
    "print(f\"Random seed: {CONFIG['seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_dataset(n_samples: int = 150) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create dummy dataset for demonstration when real data is not available.\"\"\"\n",
    "    \n",
    "    wave_types = [\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]\n",
    "    directions = [\"left\", \"right\", \"both\"]\n",
    "    confidences = [\"high\", \"medium\", \"low\"]\n",
    "    \n",
    "    items = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create realistic wave parameters with some correlations\n",
    "        wave_type = np.random.choice(wave_types)\n",
    "        direction = np.random.choice(directions)\n",
    "        \n",
    "        # Height varies by wave type\n",
    "        if wave_type == \"closeout\":\n",
    "            height = float(np.random.uniform(0.3, 1.5))\n",
    "        elif wave_type == \"reef_break\":\n",
    "            height = float(np.random.uniform(1.0, 3.0))\n",
    "        elif wave_type == \"point_break\":\n",
    "            height = float(np.random.uniform(0.8, 2.5))\n",
    "        else:\n",
    "            height = float(np.random.uniform(0.5, 2.2))\n",
    "        \n",
    "        # Confidence correlates with wave type complexity\n",
    "        if wave_type in [\"closeout\", \"beach_break\"]:\n",
    "            confidence = np.random.choice(confidences, p=[0.6, 0.3, 0.1])\n",
    "        else:\n",
    "            confidence = np.random.choice(confidences, p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        items.append({\n",
    "            \"image_path\": f\"data/dummy/images/wave_{i:03d}.jpg\",\n",
    "            \"height_meters\": round(height, 2),\n",
    "            \"wave_type\": wave_type,\n",
    "            \"direction\": direction,\n",
    "            \"confidence\": confidence,\n",
    "            \"notes\": f\"Dummy sample {i+1} - {wave_type} wave going {direction}\",\n",
    "            \"data_key\": i + 1,\n",
    "            \"source\": \"dummy\"\n",
    "        })\n",
    "    \n",
    "    return items\n",
    "\n",
    "print(\"âœ“ Dummy data generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Splitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(items, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
    "    \"\"\"Perform stratified split by wave_type and direction.\n",
    "    \n",
    "    Args:\n",
    "        items: List of dataset records\n",
    "        train_ratio: Proportion for training set\n",
    "        val_ratio: Proportion for validation set\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_items, val_items, test_items)\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # Group by wave_type and direction for stratification\n",
    "    buckets = defaultdict(list)\n",
    "    for r in items:\n",
    "        key = (r[\"wave_type\"], r[\"direction\"])\n",
    "        buckets[key].append(r)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "\n",
    "    print(f\"Stratifying by {len(buckets)} unique (wave_type, direction) combinations:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for key, bucket in buckets.items():\n",
    "        rng.shuffle(bucket)\n",
    "        n = len(bucket)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        n_train = max(1, int(round(n * train_ratio)))\n",
    "        n_val = max(0, int(round(n * val_ratio)))\n",
    "        \n",
    "        # Ensure we don't exceed bucket size\n",
    "        n_train = min(n_train, n)\n",
    "        n_val = min(n_val, n - n_train)\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        print(f\"  {key[0]:12} + {key[1]:5}: {n:3} total â†’ {n_train:3} train, {n_val:2} val, {n_test:2} test\")\n",
    "        \n",
    "        train.extend(bucket[:n_train])\n",
    "        val.extend(bucket[n_train:n_train + n_val])\n",
    "        test.extend(bucket[n_train + n_val:])\n",
    "\n",
    "    # Final shuffle to mix stratification groups\n",
    "    rng.shuffle(train)\n",
    "    rng.shuffle(val)\n",
    "    rng.shuffle(test)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "print(\"âœ“ Stratified splitting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Try to load the real dataset index\n",
    "items = read_jsonl(CONFIG[\"real_index\"])\n",
    "\n",
    "if not items and CONFIG[\"create_dummy_if_missing\"]:\n",
    "    print(f\"âš ï¸ Real dataset index not found at {CONFIG['real_index']}\")\n",
    "    print(f\"Creating dummy data for demonstration...\")\n",
    "    items = create_dummy_dataset(CONFIG[\"dummy_samples\"])\n",
    "    print(f\"âœ“ Created {len(items)} dummy samples\")\n",
    "elif items:\n",
    "    print(f\"âœ“ Loaded {len(items)} items from {CONFIG['real_index']}\")\n",
    "else:\n",
    "    raise ValueError(f\"No data found and dummy creation disabled\")\n",
    "\n",
    "if len(items) == 0:\n",
    "    raise ValueError(\"No data available for splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data distribution before splitting\n",
    "df_full = pd.DataFrame(items)\n",
    "\n",
    "print(\"\\nDataset overview before splitting:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total samples: {len(df_full)}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Height statistics:\")\n",
    "height_stats = df_full['height_meters'].describe()\n",
    "for stat, value in height_stats.items():\n",
    "    print(f\"  {stat}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸŒŠ Wave type distribution:\")\n",
    "wave_type_counts = df_full['wave_type'].value_counts()\n",
    "for wt, count in wave_type_counts.items():\n",
    "    percentage = count / len(df_full) * 100\n",
    "    print(f\"  {wt}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ§­ Direction distribution:\")\n",
    "direction_counts = df_full['direction'].value_counts()\n",
    "for direction, count in direction_counts.items():\n",
    "    percentage = count / len(df_full) * 100\n",
    "    print(f\"  {direction}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show stratification groups\n",
    "strat_keys = df_full.groupby(['wave_type', 'direction']).size()\n",
    "print(f\"\\nğŸ“Š Stratification groups (wave_type, direction):\")\n",
    "for (wt, direction), count in strat_keys.items():\n",
    "    print(f\"  {wt} + {direction}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified split\n",
    "print(f\"\\nPerforming stratified split...\")\n",
    "print(f\"Target ratios: Train={CONFIG['train_ratio']}, Val={CONFIG['val_ratio']}, Test={CONFIG['test_ratio']}\")\n",
    "print()\n",
    "\n",
    "train, val, test = stratified_split(\n",
    "    items, \n",
    "    train_ratio=CONFIG['train_ratio'], \n",
    "    val_ratio=CONFIG['val_ratio'], \n",
    "    seed=CONFIG['seed']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Split results:\")\n",
    "print(f\"Train: {len(train)} samples ({len(train)/len(items)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val)} samples ({len(val)/len(items)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test)} samples ({len(test)/len(items)*100:.1f}%)\")\n",
    "print(f\"Total: {len(train) + len(val) + len(test)} samples\")\n",
    "\n",
    "# Verify no data loss\n",
    "assert len(train) + len(val) + len(test) == len(items), \"Data loss during splitting!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory and save splits\n",
    "ensure_dir(CONFIG[\"out_dir\"])\n",
    "\n",
    "train_path = os.path.join(CONFIG[\"out_dir\"], \"train.jsonl\")\n",
    "val_path = os.path.join(CONFIG[\"out_dir\"], \"val.jsonl\")\n",
    "test_path = os.path.join(CONFIG[\"out_dir\"], \"test.jsonl\")\n",
    "\n",
    "write_jsonl(train, train_path)\n",
    "write_jsonl(val, val_path)\n",
    "write_jsonl(test, test_path)\n",
    "\n",
    "print(f\"âœ“ Saved splits to {CONFIG['out_dir']}/\")\n",
    "print(f\"  Train: {train_path} ({len(train)} samples)\")\n",
    "print(f\"  Val: {val_path} ({len(val)} samples)\")\n",
    "print(f\"  Test: {test_path} ({len(test)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert splits to DataFrames for analysis\n",
    "df_train = pd.DataFrame(train)\n",
    "df_val = pd.DataFrame(val)\n",
    "df_test = pd.DataFrame(test)\n",
    "\n",
    "# Add split labels for combined analysis\n",
    "df_train['split'] = 'train'\n",
    "df_val['split'] = 'val'\n",
    "df_test['split'] = 'test'\n",
    "\n",
    "df_combined = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "print(\"Split DataFrames created for analysis\")\n",
    "print(f\"Combined DataFrame shape: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution preservation across splits\n",
    "print(\"Distribution Analysis Across Splits:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Wave Type Distribution (% within each wave type):\")\n",
    "wave_type_dist = pd.crosstab(df_combined['wave_type'], df_combined['split'], normalize='index') * 100\n",
    "print(wave_type_dist.round(1))\n",
    "\n",
    "print(\"\\n2. Direction Distribution (% within each direction):\")\n",
    "direction_dist = pd.crosstab(df_combined['direction'], df_combined['split'], normalize='index') * 100\n",
    "print(direction_dist.round(1))\n",
    "\n",
    "print(\"\\n3. Combined (Wave Type, Direction) Distribution:\")\n",
    "df_combined['strat_key'] = df_combined['wave_type'] + '_' + df_combined['direction']\n",
    "combined_dist = pd.crosstab(df_combined['strat_key'], df_combined['split'], normalize='index') * 100\n",
    "print(combined_dist.round(1))\n",
    "\n",
    "# Check for good stratification (should be close to target ratios)\n",
    "target_train, target_val, target_test = CONFIG['train_ratio']*100, CONFIG['val_ratio']*100, CONFIG['test_ratio']*100\n",
    "print(f\"\\nTarget ratios: Train={target_train:.1f}%, Val={target_val:.1f}%, Test={target_test:.1f}%\")\n",
    "\n",
    "# Calculate average deviations\n",
    "train_dev = abs(wave_type_dist['train'] - target_train).mean()\n",
    "val_dev = abs(wave_type_dist['val'] - target_val).mean()\n",
    "test_dev = abs(wave_type_dist['test'] - target_test).mean()\n",
    "\n",
    "print(f\"Average deviations from target ratios:\")\n",
    "print(f\"  Train: Â±{train_dev:.1f}%\")\n",
    "print(f\"  Val: Â±{val_dev:.1f}%\")\n",
    "print(f\"  Test: Â±{test_dev:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Split Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize split distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Height distributions by split\n",
    "split_data = [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for i, (split_name, split_df) in enumerate(split_data):\n",
    "    axes[0, i].hist(split_df['height_meters'], bins=15, alpha=0.7, \n",
    "                   edgecolor='black', color=colors[i])\n",
    "    axes[0, i].set_title(f'{split_name} Set - Height Distribution')\n",
    "    axes[0, i].set_xlabel('Height (meters)')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "    axes[0, i].axvline(split_df['height_meters'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {split_df[\"height_meters\"].mean():.2f}m')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Wave type distributions\n",
    "wave_type_counts = [df_train['wave_type'].value_counts(), \n",
    "                   df_val['wave_type'].value_counts(), \n",
    "                   df_test['wave_type'].value_counts()]\n",
    "\n",
    "for i, (split_name, counts) in enumerate(zip([\"Train\", \"Val\", \"Test\"], wave_type_counts)):\n",
    "    bars = axes[1, i].bar(counts.index, counts.values, color=colors[i], alpha=0.7)\n",
    "    axes[1, i].set_title(f'{split_name} Set - Wave Type Distribution')\n",
    "    axes[1, i].set_xlabel('Wave Type')\n",
    "    axes[1, i].set_ylabel('Count')\n",
    "    axes[1, i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts.values):\n",
    "        axes[1, i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                       str(count), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Direction distribution across splits\n",
    "direction_counts_by_split = df_combined.groupby(['direction', 'split']).size().unstack(fill_value=0)\n",
    "direction_counts_by_split.plot(kind='bar', ax=axes[0, 0], color=colors)\n",
    "axes[0, 0].set_title('Direction Distribution Across Splits')\n",
    "axes[0, 0].set_xlabel('Direction')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].legend(title='Split')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Confidence distribution across splits\n",
    "conf_counts_by_split = df_combined.groupby(['confidence', 'split']).size().unstack(fill_value=0)\n",
    "conf_counts_by_split.plot(kind='bar', ax=axes[0, 1], color=colors)\n",
    "axes[0, 1].set_title('Confidence Distribution Across Splits')\n",
    "axes[0, 1].set_xlabel('Confidence Level')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].legend(title='Split')\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Height box plots by split\n",
    "split_names = ['train', 'val', 'test']\n",
    "height_data = [df_combined[df_combined['split'] == split]['height_meters'].values \n",
    "               for split in split_names]\n",
    "\n",
    "box_plot = axes[1, 0].boxplot(height_data, labels=['Train', 'Val', 'Test'], patch_artist=True)\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[1, 0].set_title('Height Distribution by Split (Box Plot)')\n",
    "axes[1, 0].set_xlabel('Split')\n",
    "axes[1, 0].set_ylabel('Height (meters)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Split size pie chart\n",
    "split_sizes = [len(train), len(val), len(test)]\n",
    "split_labels = [f'Train\\n({len(train)})', f'Val\\n({len(val)})', f'Test\\n({len(test)})']\n",
    "\n",
    "axes[1, 1].pie(split_sizes, labels=split_labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Split Size Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests for Split Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for distribution similarity\n",
    "print(\"Statistical Tests for Split Quality:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Kolmogorov-Smirnov test for height distributions\n",
    "ks_train_val = ks_2samp(df_train['height_meters'], df_val['height_meters'])\n",
    "ks_train_test = ks_2samp(df_train['height_meters'], df_test['height_meters'])\n",
    "ks_val_test = ks_2samp(df_val['height_meters'], df_test['height_meters'])\n",
    "\n",
    "print(f\"\\nğŸ“Š Height Distribution Similarity (KS test p-values):\")\n",
    "print(f\"  Train vs Val: {ks_train_val.pvalue:.4f}\")\n",
    "print(f\"  Train vs Test: {ks_train_test.pvalue:.4f}\")\n",
    "print(f\"  Val vs Test: {ks_val_test.pvalue:.4f}\")\n",
    "print(\"  (Higher p-values indicate more similar distributions)\")\n",
    "\n",
    "# Chi-square test for categorical distributions\n",
    "wave_type_contingency = pd.crosstab(df_combined['wave_type'], df_combined['split'])\n",
    "chi2_wt, p_wt, _, _ = chi2_contingency(wave_type_contingency)\n",
    "\n",
    "direction_contingency = pd.crosstab(df_combined['direction'], df_combined['split'])\n",
    "chi2_dir, p_dir, _, _ = chi2_contingency(direction_contingency)\n",
    "\n",
    "print(f\"\\nğŸ“Š Categorical Distribution Similarity (Chi-square test p-values):\")\n",
    "print(f\"  Wave type across splits: {p_wt:.4f}\")\n",
    "print(f\"  Direction across splits: {p_dir:.4f}\")\n",
    "print(\"  (Higher p-values indicate more similar distributions)\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "if all(p > 0.05 for p in [ks_train_val.pvalue, ks_train_test.pvalue, ks_val_test.pvalue]):\n",
    "    print(\"  âœ“ Height distributions are statistically similar across splits\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Some height distributions differ significantly between splits\")\n",
    "\n",
    "if p_wt > 0.05 and p_dir > 0.05:\n",
    "    print(\"  âœ“ Categorical distributions are well-balanced across splits\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Some categorical imbalances detected across splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Split Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed split summary\n",
    "print(\"Detailed Split Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name, split_df in [(\"TRAINING\", df_train), (\"VALIDATION\", df_val), (\"TEST\", df_test)]:\n",
    "    print(f\"\\n{split_name} SET:\")\n",
    "    print(f\"  ğŸ“Š Samples: {len(split_df)} ({len(split_df)/len(items)*100:.1f}% of total)\")\n",
    "    \n",
    "    print(f\"  ğŸ“ Height statistics:\")\n",
    "    print(f\"    Range: {split_df['height_meters'].min():.2f}m - {split_df['height_meters'].max():.2f}m\")\n",
    "    print(f\"    Mean: {split_df['height_meters'].mean():.2f}m Â± {split_df['height_meters'].std():.2f}m\")\n",
    "    print(f\"    Median: {split_df['height_meters'].median():.2f}m\")\n",
    "    \n",
    "    print(f\"  ğŸŒŠ Wave types: {dict(split_df['wave_type'].value_counts())}\")\n",
    "    print(f\"  ğŸ§­ Directions: {dict(split_df['direction'].value_counts())}\")\n",
    "    print(f\"  âš–ï¸ Confidence levels: {dict(split_df['confidence'].value_counts())}\")\n",
    "    \n",
    "    # Unique stratification groups in this split\n",
    "    strat_groups = split_df.groupby(['wave_type', 'direction']).size()\n",
    "    print(f\"  ğŸ“‹ Stratification groups: {len(strat_groups)} unique combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive validation checks\n",
    "print(\"Split Validation Checks:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check 1: No overlap between splits\n",
    "train_paths = set(r['image_path'] for r in train)\n",
    "val_paths = set(r['image_path'] for r in val)\n",
    "test_paths = set(r['image_path'] for r in test)\n",
    "\n",
    "overlap_train_val = train_paths & val_paths\n",
    "overlap_train_test = train_paths & test_paths\n",
    "overlap_val_test = val_paths & test_paths\n",
    "\n",
    "if not overlap_train_val and not overlap_train_test and not overlap_val_test:\n",
    "    print(\"âœ“ No overlap between splits - data integrity maintained\")\n",
    "else:\n",
    "    print(\"âœ— Overlap detected between splits!\")\n",
    "    if overlap_train_val:\n",
    "        print(f\"  Train-Val overlap: {len(overlap_train_val)} samples\")\n",
    "    if overlap_train_test:\n",
    "        print(f\"  Train-Test overlap: {len(overlap_train_test)} samples\")\n",
    "    if overlap_val_test:\n",
    "        print(f\"  Val-Test overlap: {len(overlap_val_test)} samples\")\n",
    "\n",
    "# Check 2: Total count preservation\n",
    "total_split = len(train) + len(val) + len(test)\n",
    "if total_split == len(items):\n",
    "    print(f\"âœ“ All {len(items)} samples accounted for in splits\")\n",
    "else:\n",
    "    print(f\"âœ— Sample count mismatch: {total_split} in splits vs {len(items)} original\")\n",
    "\n",
    "# Check 3: Minimum samples per class in each split\n",
    "min_samples_per_class = 1\n",
    "for split_name, split_df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    wt_counts = split_df['wave_type'].value_counts()\n",
    "    dir_counts = split_df['direction'].value_counts()\n",
    "    \n",
    "    if wt_counts.min() >= min_samples_per_class and dir_counts.min() >= min_samples_per_class:\n",
    "        print(f\"âœ“ {split_name.capitalize()} split has sufficient samples per class\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {split_name.capitalize()} split has classes with few samples:\")\n",
    "        if wt_counts.min() < min_samples_per_class:\n",
    "            sparse_wt = wt_counts[wt_counts < min_samples_per_class]\n",
    "            print(f\"    Wave types: {sparse_wt.to_dict()}\")\n",
    "        if dir_counts.min() < min_samples_per_class:\n",
    "            sparse_dir = dir_counts[dir_counts < min_samples_per_class]\n",
    "            print(f\"    Directions: {sparse_dir.to_dict()}\")\n",
    "\n",
    "# Check 4: Split ratio validation\n",
    "actual_ratios = {\n",
    "    'train': len(train) / len(items),\n",
    "    'val': len(val) / len(items),\n",
    "    'test': len(test) / len(items)\n",
    "}\n",
    "\n",
    "target_ratios = {\n",
    "    'train': CONFIG['train_ratio'],\n",
    "    'val': CONFIG['val_ratio'],\n",
    "    'test': CONFIG['test_ratio']\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š Split ratio validation:\")\n",
    "ratio_ok = True\n",
    "for split in ['train', 'val', 'test']:\n",
    "    actual = actual_ratios[split]\n",
    "    target = target_ratios[split]\n",
    "    diff = abs(actual - target)\n",
    "    \n",
    "    if diff < 0.05:  # Within 5% tolerance\n",
    "        print(f\"  âœ“ {split.capitalize()}: {actual:.3f} (target: {target:.3f}, diff: {diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ {split.capitalize()}: {actual:.3f} (target: {target:.3f}, diff: {diff:.3f})\")\n",
    "        ratio_ok = False\n",
    "\n",
    "if ratio_ok:\n",
    "    print(\"âœ“ All split ratios are within acceptable tolerance\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some split ratios deviate significantly from targets\")\n",
    "\n",
    "print(f\"\\nâœ… Splits validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Split Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive split statistics\n",
    "split_stats = {\n",
    "    \"config\": CONFIG,\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(items),\n",
    "        \"source\": \"real_data\" if not CONFIG[\"create_dummy_if_missing\"] or os.path.exists(CONFIG[\"real_index\"]) else \"dummy_data\",\n",
    "        \"stratification_groups\": len(strat_keys)\n",
    "    },\n",
    "    \"split_sizes\": {\n",
    "        \"train\": len(train),\n",
    "        \"val\": len(val),\n",
    "        \"test\": len(test)\n",
    "    },\n",
    "    \"split_ratios\": {\n",
    "        \"actual\": actual_ratios,\n",
    "        \"target\": target_ratios\n",
    "    },\n",
    "    \"height_stats\": {\n",
    "        \"train\": df_train['height_meters'].describe().to_dict(),\n",
    "        \"val\": df_val['height_meters'].describe().to_dict(),\n",
    "        \"test\": df_test['height_meters'].describe().to_dict()\n",
    "    },\n",
    "    \"wave_type_distribution\": {\n",
    "        \"train\": df_train['wave_type'].value_counts().to_dict(),\n",
    "        \"val\": df_val['wave_type'].value_counts().to_dict(),\n",
    "        \"test\": df_test['wave_type'].value_counts().to_dict()\n",
    "    },\n",
    "    \"direction_distribution\": {\n",
    "        \"train\": df_train['direction'].value_counts().to_dict(),\n",
    "        \"val\": df_val['direction'].value_counts().to_dict(),\n",
    "        \"test\": df_test['direction'].value_counts().to_dict()\n",
    "    },\n",
    "    \"confidence_distribution\": {\n",
    "        \"train\": df_train['confidence'].value_counts().to_dict(),\n",
    "        \"val\": df_val['confidence'].value_counts().to_dict(),\n",
    "        \"test\": df_test['confidence'].value_counts().to_dict()\n",
    "    },\n",
    "    \"cross_tabulation\": {\n",
    "        \"wave_type_vs_direction\": pd.crosstab(df_full['wave_type'], df_full['direction']).to_dict()\n",
    "    },\n",
    "    \"statistical_tests\": {\n",
    "        \"ks_tests\": {\n",
    "            \"train_vs_val\": ks_train_val.pvalue,\n",
    "            \"train_vs_test\": ks_train_test.pvalue,\n",
    "            \"val_vs_test\": ks_val_test.pvalue\n",
    "        },\n",
    "        \"chi2_tests\": {\n",
    "            \"wave_type\": p_wt,\n",
    "            \"direction\": p_dir\n",
    "        }\n",
    "    },\n",
    "    \"validation_results\": {\n",
    "        \"no_overlap\": len(overlap_train_val) == 0 and len(overlap_train_test) == 0 and len(overlap_val_test) == 0,\n",
    "        \"count_preserved\": total_split == len(items),\n",
    "        \"ratios_within_tolerance\": ratio_ok\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save statistics\n",
    "stats_path = os.path.join(CONFIG[\"out_dir\"], \"split_statistics.json\")\n",
    "with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(split_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save summary CSV\n",
    "summary_df = pd.DataFrame([\n",
    "    {'split': 'train', 'samples': len(train), 'ratio': actual_ratios['train']},\n",
    "    {'split': 'val', 'samples': len(val), 'ratio': actual_ratios['val']},\n",
    "    {'split': 'test', 'samples': len(test), 'ratio': actual_ratios['test']}\n",
    "])\n",
    "summary_df.to_csv(os.path.join(CONFIG[\"out_dir\"], \"split_summary.csv\"), index=False)\n",
    "\n",
    "print(f\"âœ… Statistics and summaries saved:\")\n",
    "print(f\"  ğŸ“Š Detailed statistics: {stats_path}\")\n",
    "print(f\"  ğŸ“‹ Summary CSV: {os.path.join(CONFIG['out_dir'], 'split_summary.csv')}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Final Summary:\")\n",
    "print(f\"  Total samples: {len(items)}\")\n",
    "print(f\"  Stratification groups: {len(strat_keys)}\")\n",
    "print(f\"  Train: {len(train)} samples ({actual_ratios['train']:.1%})\")\n",
    "print(f\"  Val: {len(val)} samples ({actual_ratios['val']:.1%})\")\n",
    "print(f\"  Test: {len(test)} samples ({actual_ratios['test']:.1%})\")\n",
    "print(f\"  Output directory: {CONFIG['out_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete, standalone system for creating stratified dataset splits:\n",
    "\n",
    "### âœ… **What we accomplished:**\n",
    "1. **Flexible Data Loading**: Works with real data or creates dummy data for demonstration\n",
    "2. **Stratified Splitting**: Ensures balanced representation across wave types and directions\n",
    "3. **Quality Validation**: Comprehensive checks for data integrity and distribution balance\n",
    "4. **Statistical Analysis**: KS tests and Chi-square tests for distribution similarity\n",
    "5. **Rich Visualizations**: Distribution plots, box plots, and comparative analysis\n",
    "6. **Comprehensive Export**: Multiple output formats with detailed statistics\n",
    "\n",
    "### ğŸ”§ **Key Features:**\n",
    "- **Stratified Sampling**: Maintains class balance across all splits\n",
    "- **Robust Validation**: Multiple checks for split quality and integrity\n",
    "- **Statistical Testing**: Quantitative validation of distribution similarity\n",
    "- **Flexible Configuration**: Easy to adjust ratios and parameters\n",
    "- **Dummy Data Support**: Works without real data for testing and development\n",
    "\n",
    "### ğŸ“Š **Split Quality Metrics:**\n",
    "- **No Data Leakage**: Verified no overlap between train/val/test sets\n",
    "- **Balanced Representation**: Each class appears in all splits proportionally\n",
    "- **Statistical Similarity**: Distributions are statistically similar across splits\n",
    "- **Ratio Compliance**: Actual ratios match target ratios within tolerance\n",
    "\n",
    "### ğŸš€ **Usage:**\n",
    "- **With Real Data**: Point to your dataset index JSONL file\n",
    "- **Without Real Data**: Automatic dummy data generation for testing\n",
    "- **Customizable**: Adjust split ratios, stratification keys, and validation criteria\n",
    "- **Production Ready**: Comprehensive validation and error handling\n",
    "\n",
    "### ğŸ“ **Output Files:**\n",
    "- `train.jsonl`, `val.jsonl`, `test.jsonl`: Split dataset files\n",
    "- `split_statistics.json`: Comprehensive statistics and validation results\n",
    "- `split_summary.csv`: Quick summary table\n",
    "\n",
    "**This notebook is completely standalone and ensures high-quality, balanced dataset splits for machine learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}