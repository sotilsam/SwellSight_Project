{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwellSight Inference\n",
    "\n",
    "This notebook demonstrates how to use trained SwellSight models for wave analysis inference.\n",
    "\n",
    "## Purpose\n",
    "- Load trained models and vocabularies\n",
    "- Run inference on single images\n",
    "- Batch inference on multiple images\n",
    "- Visualize predictions and confidence\n",
    "- Error analysis and model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import components from previous notebooks\n",
    "%run 01_model_architecture.ipynb\n",
    "%run 02_data_loading.ipynb\n",
    "%run 03_loss_and_metrics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INFERENCE_CONFIG = {\n",
    "    \"model_path\": \"runs/swell_real/best.pt\",  # Update this path\n",
    "    \"vocabs_path\": \"runs/swell_real/vocabs.json\",  # Update this path\n",
    "    \"image_size\": 224,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "print(\"Inference Configuration:\")\n",
    "for key, value in INFERENCE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_vocabs(model_path: str, vocabs_path: str, device: str):\n",
    "    \"\"\"Load trained model and vocabularies.\"\"\"\n",
    "    \n",
    "    # Load vocabularies\n",
    "    try:\n",
    "        with open(vocabs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocabs = json.load(f)\n",
    "        wt2id = vocabs[\"wave_type_to_id\"]\n",
    "        d2id = vocabs[\"direction_to_id\"]\n",
    "        print(f\"✓ Loaded vocabularies from {vocabs_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ Vocabularies not found: {vocabs_path}\")\n",
    "        print(\"Using default vocabularies...\")\n",
    "        wt2id = {\"beach_break\": 0, \"reef_break\": 1, \"point_break\": 2, \"closeout\": 3, \"a_frame\": 4}\n",
    "        d2id = {\"left\": 0, \"right\": 1, \"both\": 2}\n",
    "        vocabs = {\"wave_type_to_id\": wt2id, \"direction_to_id\": d2id}\n",
    "    \n",
    "    # Create reverse mappings\n",
    "    id2wt = {v: k for k, v in wt2id.items()}\n",
    "    id2d = {v: k for k, v in d2id.items()}\n",
    "    \n",
    "    # Create model\n",
    "    model = SwellSightNet(len(wt2id), len(d2id)).to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        model.eval()\n",
    "        print(f\"✓ Loaded model from {model_path}\")\n",
    "        if \"epoch\" in checkpoint:\n",
    "            print(f\"  Model from epoch {checkpoint['epoch']}\")\n",
    "        if \"val_loss\" in checkpoint:\n",
    "            print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ Model not found: {model_path}\")\n",
    "        print(\"Using randomly initialized model for demonstration...\")\n",
    "        model.eval()\n",
    "    \n",
    "    return model, vocabs, id2wt, id2d\n",
    "\n",
    "# Load model and vocabularies\n",
    "model, vocabs, id2wt, id2d = load_model_and_vocabs(\n",
    "    INFERENCE_CONFIG[\"model_path\"],\n",
    "    INFERENCE_CONFIG[\"vocabs_path\"],\n",
    "    INFERENCE_CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded with:\")\n",
    "print(f\"  Wave types: {list(id2wt.values())}\")\n",
    "print(f\"  Directions: {list(id2d.values())}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str, image_size: int = 224) -> torch.Tensor:\n",
    "    \"\"\"Preprocess image for inference.\"\"\"\n",
    "    transform = build_infer_transform(image_size)\n",
    "    \n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_path  # Already a PIL Image\n",
    "    \n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_single_image(\n",
    "    model: torch.nn.Module,\n",
    "    image_path: str,\n",
    "    id2wt: Dict[int, str],\n",
    "    id2d: Dict[int, str],\n",
    "    device: str,\n",
    "    image_size: int = 224\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"Run inference on a single image.\"\"\"\n",
    "    \n",
    "    # Preprocess image\n",
    "    x = preprocess_image(image_path, image_size).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    pred_h, pred_wt, pred_dir = model(x)\n",
    "    \n",
    "    # Convert predictions to interpretable format\n",
    "    height_m = float(pred_h.item())\n",
    "    \n",
    "    # Get class probabilities\n",
    "    wt_probs = torch.softmax(pred_wt, dim=1).cpu().numpy()[0]\n",
    "    dir_probs = torch.softmax(pred_dir, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Get predicted classes\n",
    "    wt_pred_id = int(torch.argmax(pred_wt, dim=1).item())\n",
    "    dir_pred_id = int(torch.argmax(pred_dir, dim=1).item())\n",
    "    \n",
    "    wave_type = id2wt[wt_pred_id]\n",
    "    direction = id2d[dir_pred_id]\n",
    "    \n",
    "    # Get confidence scores\n",
    "    wt_confidence = float(wt_probs[wt_pred_id])\n",
    "    dir_confidence = float(dir_probs[dir_pred_id])\n",
    "    \n",
    "    return {\n",
    "        \"height_meters\": height_m,\n",
    "        \"wave_type\": wave_type,\n",
    "        \"direction\": direction,\n",
    "        \"wave_type_confidence\": wt_confidence,\n",
    "        \"direction_confidence\": dir_confidence,\n",
    "        \"wave_type_probs\": {id2wt[i]: float(prob) for i, prob in enumerate(wt_probs)},\n",
    "        \"direction_probs\": {id2d[i]: float(prob) for i, prob in enumerate(dir_probs)},\n",
    "        \"raw_predictions\": {\n",
    "            \"height\": pred_h.cpu().numpy(),\n",
    "            \"wave_type_logits\": pred_wt.cpu().numpy(),\n",
    "            \"direction_logits\": pred_dir.cpu().numpy()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def create_dummy_image(size: Tuple[int, int] = (512, 512), wave_params: Dict = None) -> Image.Image:\n",
    "    \"\"\"Create a dummy wave image for demonstration.\"\"\"\n",
    "    # Create a simple gradient that looks like water\n",
    "    img = Image.new('RGB', size, color='lightblue')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Add some wave-like patterns\n",
    "    for y in range(0, size[1], 20):\n",
    "        wave_offset = int(10 * np.sin(y * 0.1))\n",
    "        color_intensity = int(200 + 30 * np.sin(y * 0.05))\n",
    "        color = (100, 150, color_intensity)\n",
    "        draw.line([(0, y + wave_offset), (size[0], y + wave_offset)], fill=color, width=3)\n",
    "    \n",
    "    # Add text if wave parameters provided\n",
    "    if wave_params:\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        text = f\"Dummy Wave\\nHeight: {wave_params.get('height', 1.5):.1f}m\\nType: {wave_params.get('type', 'beach_break')}\"\n",
    "        draw.text((10, 10), text, fill='white', font=font)\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(\"✓ Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load a demo image\n",
    "demo_image_path = \"demo_wave.jpg\"\n",
    "\n",
    "# Check if we have a real image, otherwise create a dummy one\n",
    "if not os.path.exists(demo_image_path):\n",
    "    print(\"Creating dummy wave image for demonstration...\")\n",
    "    dummy_params = {\"height\": 1.8, \"type\": \"reef_break\", \"direction\": \"left\"}\n",
    "    demo_image = create_dummy_image(wave_params=dummy_params)\n",
    "    demo_image.save(demo_image_path)\n",
    "    print(f\"✓ Saved dummy image to {demo_image_path}\")\n",
    "else:\n",
    "    demo_image = Image.open(demo_image_path)\n",
    "    print(f\"✓ Using existing image: {demo_image_path}\")\n",
    "\n",
    "# Display the demo image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(demo_image)\n",
    "plt.title(\"Demo Wave Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {demo_image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the demo image\n",
    "print(\"Running inference on demo image...\")\n",
    "\n",
    "prediction = predict_single_image(\n",
    "    model=model,\n",
    "    image_path=demo_image_path,\n",
    "    id2wt=id2wt,\n",
    "    id2d=id2d,\n",
    "    device=INFERENCE_CONFIG[\"device\"],\n",
    "    image_size=INFERENCE_CONFIG[\"image_size\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WAVE ANALYSIS RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Wave Height: {prediction['height_meters']:.2f} meters\")\n",
    "print(f\"Wave Type: {prediction['wave_type']} (confidence: {prediction['wave_type_confidence']:.3f})\")\n",
    "print(f\"Direction: {prediction['direction']} (confidence: {prediction['direction_confidence']:.3f})\")\n",
    "\n",
    "print(f\"\\nDetailed Wave Type Probabilities:\")\n",
    "for wt, prob in prediction['wave_type_probs'].items():\n",
    "    print(f\"  {wt}: {prob:.3f}\")\n",
    "\n",
    "print(f\"\\nDetailed Direction Probabilities:\")\n",
    "for direction, prob in prediction['direction_probs'].items():\n",
    "    print(f\"  {direction}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(image_path: str, prediction: Dict, figsize: Tuple[int, int] = (15, 10)):\n",
    "    \"\"\"Visualize prediction results with probability distributions.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Original image\n",
    "    if isinstance(image_path, str):\n",
    "        img = Image.open(image_path)\n",
    "    else:\n",
    "        img = image_path\n",
    "    \n",
    "    axes[0, 0].imshow(img)\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Wave type probabilities\n",
    "    wt_names = list(prediction['wave_type_probs'].keys())\n",
    "    wt_probs = list(prediction['wave_type_probs'].values())\n",
    "    \n",
    "    bars1 = axes[0, 1].bar(wt_names, wt_probs, color='skyblue', alpha=0.7)\n",
    "    axes[0, 1].set_title('Wave Type Probabilities')\n",
    "    axes[0, 1].set_ylabel('Probability')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Highlight predicted class\n",
    "    pred_wt_idx = wt_names.index(prediction['wave_type'])\n",
    "    bars1[pred_wt_idx].set_color('orange')\n",
    "    bars1[pred_wt_idx].set_alpha(1.0)\n",
    "    \n",
    "    # Add probability values on bars\n",
    "    for i, (bar, prob) in enumerate(zip(bars1, wt_probs)):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{prob:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Direction probabilities\n",
    "    dir_names = list(prediction['direction_probs'].keys())\n",
    "    dir_probs = list(prediction['direction_probs'].values())\n",
    "    \n",
    "    bars2 = axes[1, 0].bar(dir_names, dir_probs, color='lightcoral', alpha=0.7)\n",
    "    axes[1, 0].set_title('Direction Probabilities')\n",
    "    axes[1, 0].set_ylabel('Probability')\n",
    "    \n",
    "    # Highlight predicted class\n",
    "    pred_dir_idx = dir_names.index(prediction['direction'])\n",
    "    bars2[pred_dir_idx].set_color('red')\n",
    "    bars2[pred_dir_idx].set_alpha(1.0)\n",
    "    \n",
    "    # Add probability values on bars\n",
    "    for i, (bar, prob) in enumerate(zip(bars2, dir_probs)):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{prob:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Summary text\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "PREDICTION SUMMARY\n",
    "{'='*30}\n",
    "\n",
    "Wave Height: {prediction['height_meters']:.2f} meters\n",
    "\n",
    "Wave Type: {prediction['wave_type']}\n",
    "Confidence: {prediction['wave_type_confidence']:.1%}\n",
    "\n",
    "Direction: {prediction['direction']}\n",
    "Confidence: {prediction['direction_confidence']:.1%}\n",
    "\n",
    "Overall Confidence:\n",
    "{(prediction['wave_type_confidence'] + prediction['direction_confidence'])/2:.1%}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the demo prediction\n",
    "visualize_prediction(demo_image_path, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_demo_images(n_images: int = 6) -> List[str]:\n",
    "    \"\"\"Create a batch of demo images with different characteristics.\"\"\"\n",
    "    \n",
    "    demo_params = [\n",
    "        {\"height\": 0.8, \"type\": \"beach_break\", \"direction\": \"both\"},\n",
    "        {\"height\": 1.5, \"type\": \"reef_break\", \"direction\": \"left\"},\n",
    "        {\"height\": 2.2, \"type\": \"point_break\", \"direction\": \"right\"},\n",
    "        {\"height\": 1.0, \"type\": \"closeout\", \"direction\": \"both\"},\n",
    "        {\"height\": 1.8, \"type\": \"a_frame\", \"direction\": \"both\"},\n",
    "        {\"height\": 2.5, \"type\": \"reef_break\", \"direction\": \"left\"},\n",
    "    ]\n",
    "    \n",
    "    image_paths = []\n",
    "    \n",
    "    for i, params in enumerate(demo_params[:n_images]):\n",
    "        image_path = f\"demo_batch_{i+1}.jpg\"\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            demo_image = create_dummy_image(wave_params=params)\n",
    "            demo_image.save(image_path)\n",
    "        \n",
    "        image_paths.append(image_path)\n",
    "    \n",
    "    return image_paths, demo_params[:n_images]\n",
    "\n",
    "\n",
    "def batch_inference(image_paths: List[str], model, id2wt, id2d, device, image_size=224) -> List[Dict]:\n",
    "    \"\"\"Run inference on a batch of images.\"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        try:\n",
    "            pred = predict_single_image(model, image_path, id2wt, id2d, device, image_size)\n",
    "            pred['image_path'] = image_path\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Create batch demo images\n",
    "print(\"Creating batch of demo images...\")\n",
    "batch_image_paths, batch_true_params = create_batch_demo_images(6)\n",
    "print(f\"✓ Created {len(batch_image_paths)} demo images\")\n",
    "\n",
    "# Run batch inference\n",
    "print(\"\\nRunning batch inference...\")\n",
    "batch_predictions = batch_inference(\n",
    "    batch_image_paths, model, id2wt, id2d, INFERENCE_CONFIG[\"device\"]\n",
    ")\n",
    "print(f\"✓ Processed {len(batch_predictions)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch results\n",
    "def visualize_batch_results(predictions: List[Dict], true_params: List[Dict] = None):\n",
    "    \"\"\"Visualize batch inference results.\"\"\"\n",
    "    \n",
    "    n_images = len(predictions)\n",
    "    cols = 3\n",
    "    rows = (n_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        row, col = i // cols, i % cols\n",
    "        \n",
    "        # Load and display image\n",
    "        img = Image.open(pred['image_path'])\n",
    "        axes[row, col].imshow(img)\n",
    "        \n",
    "        # Create title with predictions\n",
    "        title = f\"Predicted:\\nH: {pred['height_meters']:.1f}m\\n\"\n",
    "        title += f\"Type: {pred['wave_type']} ({pred['wave_type_confidence']:.2f})\\n\"\n",
    "        title += f\"Dir: {pred['direction']} ({pred['direction_confidence']:.2f})\"\n",
    "        \n",
    "        # Add true values if available\n",
    "        if true_params and i < len(true_params):\n",
    "            true = true_params[i]\n",
    "            title += f\"\\n\\nTrue:\\nH: {true['height']:.1f}m\\n\"\n",
    "            title += f\"Type: {true['type']}\\nDir: {true['direction']}\"\n",
    "        \n",
    "        axes[row, col].set_title(title, fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_images, rows * cols):\n",
    "        row, col = i // cols, i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize batch results\n",
    "visualize_batch_results(batch_predictions, batch_true_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "batch_df = pd.DataFrame([\n",
    "    {\n",
    "        'image': os.path.basename(pred['image_path']),\n",
    "        'pred_height': pred['height_meters'],\n",
    "        'pred_wave_type': pred['wave_type'],\n",
    "        'pred_direction': pred['direction'],\n",
    "        'wt_confidence': pred['wave_type_confidence'],\n",
    "        'dir_confidence': pred['direction_confidence'],\n",
    "        'true_height': batch_true_params[i]['height'] if i < len(batch_true_params) else None,\n",
    "        'true_wave_type': batch_true_params[i]['type'] if i < len(batch_true_params) else None,\n",
    "        'true_direction': batch_true_params[i]['direction'] if i < len(batch_true_params) else None,\n",
    "    }\n",
    "    for i, pred in enumerate(batch_predictions)\n",
    "])\n",
    "\n",
    "print(\"Batch Inference Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(batch_df.to_string(index=False))\n",
    "\n",
    "# Calculate accuracy if we have true labels\n",
    "if 'true_wave_type' in batch_df.columns and batch_df['true_wave_type'].notna().any():\n",
    "    wt_accuracy = (batch_df['pred_wave_type'] == batch_df['true_wave_type']).mean()\n",
    "    dir_accuracy = (batch_df['pred_direction'] == batch_df['true_direction']).mean()\n",
    "    \n",
    "    height_mae = np.abs(batch_df['pred_height'] - batch_df['true_height']).mean()\n",
    "    \n",
    "    print(f\"\\nBatch Performance:\")\n",
    "    print(f\"Wave Type Accuracy: {wt_accuracy:.2%}\")\n",
    "    print(f\"Direction Accuracy: {dir_accuracy:.2%}\")\n",
    "    print(f\"Height MAE: {height_mae:.3f}m\")\n",
    "\n",
    "# Confidence statistics\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"Wave Type Confidence - Mean: {batch_df['wt_confidence'].mean():.3f}, Std: {batch_df['wt_confidence'].std():.3f}\")\n",
    "print(f\"Direction Confidence - Mean: {batch_df['dir_confidence'].mean():.3f}, Std: {batch_df['dir_confidence'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Height predictions vs true (if available)\n",
    "if 'true_height' in batch_df.columns and batch_df['true_height'].notna().any():\n",
    "    axes[0].scatter(batch_df['true_height'], batch_df['pred_height'], alpha=0.7, s=100)\n",
    "    min_h, max_h = batch_df[['true_height', 'pred_height']].min().min(), batch_df[['true_height', 'pred_height']].max().max()\n",
    "    axes[0].plot([min_h, max_h], [min_h, max_h], 'r--', alpha=0.8)\n",
    "    axes[0].set_xlabel('True Height (m)')\n",
    "    axes[0].set_ylabel('Predicted Height (m)')\n",
    "    axes[0].set_title('Height Predictions')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0].hist(batch_df['pred_height'], bins=10, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Predicted Height (m)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Height Prediction Distribution')\n",
    "\n",
    "# Wave type confidence\n",
    "axes[1].hist(batch_df['wt_confidence'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Wave Type Confidence')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Wave Type Confidence Distribution')\n",
    "axes[1].axvline(batch_df['wt_confidence'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {batch_df[\"wt_confidence\"].mean():.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Direction confidence\n",
    "axes[2].hist(batch_df['dir_confidence'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[2].set_xlabel('Direction Confidence')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Direction Confidence Distribution')\n",
    "axes[2].axvline(batch_df['dir_confidence'].mean(), color='red', linestyle='--',\n",
    "               label=f'Mean: {batch_df[\"dir_confidence\"].mean():.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_uncertainty(predictions: List[Dict]):\n",
    "    \"\"\"Analyze prediction uncertainty and confidence patterns.\"\"\"\n",
    "    \n",
    "    # Extract confidence scores\n",
    "    wt_confidences = [p['wave_type_confidence'] for p in predictions]\n",
    "    dir_confidences = [p['direction_confidence'] for p in predictions]\n",
    "    \n",
    "    # Calculate entropy for each prediction (measure of uncertainty)\n",
    "    wt_entropies = []\n",
    "    dir_entropies = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        # Wave type entropy\n",
    "        wt_probs = np.array(list(pred['wave_type_probs'].values()))\n",
    "        wt_entropy = -np.sum(wt_probs * np.log(wt_probs + 1e-8))\n",
    "        wt_entropies.append(wt_entropy)\n",
    "        \n",
    "        # Direction entropy\n",
    "        dir_probs = np.array(list(pred['direction_probs'].values()))\n",
    "        dir_entropy = -np.sum(dir_probs * np.log(dir_probs + 1e-8))\n",
    "        dir_entropies.append(dir_entropy)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Confidence vs Entropy scatter plots\n",
    "    axes[0, 0].scatter(wt_confidences, wt_entropies, alpha=0.7, s=100)\n",
    "    axes[0, 0].set_xlabel('Wave Type Confidence')\n",
    "    axes[0, 0].set_ylabel('Wave Type Entropy')\n",
    "    axes[0, 0].set_title('Confidence vs Uncertainty (Wave Type)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].scatter(dir_confidences, dir_entropies, alpha=0.7, s=100, color='orange')\n",
    "    axes[0, 1].set_xlabel('Direction Confidence')\n",
    "    axes[0, 1].set_ylabel('Direction Entropy')\n",
    "    axes[0, 1].set_title('Confidence vs Uncertainty (Direction)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of predictions by class\n",
    "    wt_counts = pd.Series([p['wave_type'] for p in predictions]).value_counts()\n",
    "    axes[1, 0].bar(wt_counts.index, wt_counts.values, alpha=0.7)\n",
    "    axes[1, 0].set_title('Wave Type Prediction Distribution')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    dir_counts = pd.Series([p['direction'] for p in predictions]).value_counts()\n",
    "    axes[1, 1].bar(dir_counts.index, dir_counts.values, alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Direction Prediction Distribution')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Uncertainty Analysis:\")\n",
    "    print(f\"Wave Type - Avg Confidence: {np.mean(wt_confidences):.3f}, Avg Entropy: {np.mean(wt_entropies):.3f}\")\n",
    "    print(f\"Direction - Avg Confidence: {np.mean(dir_confidences):.3f}, Avg Entropy: {np.mean(dir_entropies):.3f}\")\n",
    "    \n",
    "    # Find most/least confident predictions\n",
    "    most_confident_wt = np.argmax(wt_confidences)\n",
    "    least_confident_wt = np.argmin(wt_confidences)\n",
    "    \n",
    "    print(f\"\\nMost confident wave type prediction: {predictions[most_confident_wt]['wave_type']} ({wt_confidences[most_confident_wt]:.3f})\")\n",
    "    print(f\"Least confident wave type prediction: {predictions[least_confident_wt]['wave_type']} ({wt_confidences[least_confident_wt]:.3f})\")\n",
    "\n",
    "# Analyze prediction uncertainty\n",
    "analyze_prediction_uncertainty(batch_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch results to file\n",
    "results_dir = \"inference_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save detailed predictions\n",
    "detailed_results = {\n",
    "    \"config\": INFERENCE_CONFIG,\n",
    "    \"model_info\": {\n",
    "        \"wave_types\": list(id2wt.values()),\n",
    "        \"directions\": list(id2d.values()),\n",
    "        \"parameters\": sum(p.numel() for p in model.parameters())\n",
    "    },\n",
    "    \"predictions\": batch_predictions,\n",
    "    \"summary_stats\": {\n",
    "        \"n_predictions\": len(batch_predictions),\n",
    "        \"avg_wt_confidence\": float(np.mean([p['wave_type_confidence'] for p in batch_predictions])),\n",
    "        \"avg_dir_confidence\": float(np.mean([p['direction_confidence'] for p in batch_predictions])),\n",
    "        \"height_range\": [float(min(p['height_meters'] for p in batch_predictions)),\n",
    "                        float(max(p['height_meters'] for p in batch_predictions))]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(results_dir, \"batch_predictions.json\"), \"w\") as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "\n",
    "# Save CSV summary\n",
    "batch_df.to_csv(os.path.join(results_dir, \"batch_summary.csv\"), index=False)\n",
    "\n",
    "print(f\"✓ Results saved to {results_dir}/\")\n",
    "print(f\"  - batch_predictions.json: Detailed predictions with probabilities\")\n",
    "print(f\"  - batch_summary.csv: Summary table\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE SESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {INFERENCE_CONFIG['model_path']}\")\n",
    "print(f\"Device: {INFERENCE_CONFIG['device']}\")\n",
    "print(f\"Images processed: {len(batch_predictions)}\")\n",
    "print(f\"Average wave type confidence: {np.mean([p['wave_type_confidence'] for p in batch_predictions]):.3f}\")\n",
    "print(f\"Average direction confidence: {np.mean([p['direction_confidence'] for p in batch_predictions]):.3f}\")\n",
    "print(f\"Height range: {min(p['height_meters'] for p in batch_predictions):.2f}m - {max(p['height_meters'] for p in batch_predictions):.2f}m\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates comprehensive inference capabilities for the SwellSight model:\n",
    "\n",
    "1. **Model Loading**: Load trained models and vocabularies with error handling\n",
    "2. **Single Image Inference**: Predict wave parameters for individual images\n",
    "3. **Batch Processing**: Efficient inference on multiple images\n",
    "4. **Visualization**: Rich visualizations of predictions and confidence scores\n",
    "5. **Uncertainty Analysis**: Analyze model confidence and prediction uncertainty\n",
    "6. **Results Export**: Save predictions in multiple formats for further analysis\n",
    "\n",
    "The inference pipeline provides:\n",
    "- Wave height predictions in meters\n",
    "- Wave type classification with confidence scores\n",
    "- Direction classification with confidence scores\n",
    "- Detailed probability distributions for all classes\n",
    "- Uncertainty quantification through entropy analysis\n",
    "\n",
    "This notebook can be adapted for production use by:\n",
    "- Adding real image loading and preprocessing\n",
    "- Implementing GPU batch processing for efficiency\n",
    "- Adding model ensemble capabilities\n",
    "- Integrating with web APIs or mobile applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}