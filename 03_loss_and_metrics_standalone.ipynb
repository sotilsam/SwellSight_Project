{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwellSight Loss Functions and Metrics (Standalone)\n",
    "\n",
    "This notebook defines the multi-task loss function and evaluation metrics for the SwellSight model.\n",
    "**This is a standalone version that includes all necessary code and doesn't require external files.**\n",
    "\n",
    "## Components\n",
    "- Multi-task loss combining regression and classification\n",
    "- Regression metrics (MAE, MSE, RMSE)\n",
    "- Classification metrics (accuracy, macro F1)\n",
    "- Sample weighting based on confidence\n",
    "- Comprehensive visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not available\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "required_packages = ['torch', 'matplotlib', 'seaborn', 'numpy', 'scikit-learn']\n",
    "for package in required_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "print(\"âœ“ All required packages are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"Multi-task loss combining regression and classification losses.\n",
    "    \n",
    "    Combines:\n",
    "    - Height regression loss (SmoothL1Loss)\n",
    "    - Wave type classification loss (CrossEntropyLoss)\n",
    "    - Direction classification loss (CrossEntropyLoss)\n",
    "    \n",
    "    All losses are weighted by sample confidence and task importance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, w_height=1.0, w_wave_type=1.0, w_direction=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            w_height: Weight for height regression loss\n",
    "            w_wave_type: Weight for wave type classification loss\n",
    "            w_direction: Weight for direction classification loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w_height = w_height\n",
    "        self.w_wave_type = w_wave_type\n",
    "        self.w_direction = w_direction\n",
    "\n",
    "        # Use SmoothL1Loss for height (robust to outliers)\n",
    "        self.reg = nn.SmoothL1Loss(reduction=\"none\")\n",
    "        # Use CrossEntropyLoss for classifications\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, sample_w):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_h: Height predictions (batch_size, 1)\n",
    "            pred_wt: Wave type logits (batch_size, num_wave_types)\n",
    "            pred_dir: Direction logits (batch_size, num_directions)\n",
    "            y_h: True heights (batch_size, 1)\n",
    "            y_wt: True wave type labels (batch_size,)\n",
    "            y_dir: True direction labels (batch_size,)\n",
    "            sample_w: Sample weights (batch_size, 1)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (total_loss, loss_dict)\n",
    "        \"\"\"\n",
    "        w = sample_w.view(-1)\n",
    "\n",
    "        # Height regression loss\n",
    "        loss_h = self.reg(pred_h.view(-1), y_h.view(-1))\n",
    "        loss_h = (loss_h * w).mean()\n",
    "\n",
    "        # Wave type classification loss\n",
    "        loss_wt = self.ce(pred_wt, y_wt.view(-1))\n",
    "        loss_wt = (loss_wt * w).mean()\n",
    "\n",
    "        # Direction classification loss\n",
    "        loss_dir = self.ce(pred_dir, y_dir.view(-1))\n",
    "        loss_dir = (loss_dir * w).mean()\n",
    "\n",
    "        # Combine losses with task weights\n",
    "        total = self.w_height * loss_h + self.w_wave_type * loss_wt + self.w_direction * loss_dir\n",
    "        \n",
    "        loss_dict = {\n",
    "            \"loss_h\": float(loss_h.item()), \n",
    "            \"loss_wt\": float(loss_wt.item()), \n",
    "            \"loss_dir\": float(loss_dir.item()),\n",
    "            \"total_loss\": float(total.item())\n",
    "        }\n",
    "        \n",
    "        return total, loss_dict\n",
    "\n",
    "print(\"âœ“ MultiTaskLoss class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(y_true: torch.Tensor, y_pred: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Compute common regression metrics for wave height prediction.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values (N,) or (N, 1)\n",
    "        y_pred: Predicted values (N,) or (N, 1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with mae, mse, rmse, r2\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().float().view(-1)\n",
    "    y_pred = y_pred.detach().float().view(-1)\n",
    "\n",
    "    # Basic error metrics\n",
    "    err = y_pred - y_true\n",
    "    mae = err.abs().mean()\n",
    "    mse = (err ** 2).mean()\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # R-squared (coefficient of determination)\n",
    "    ss_res = ((y_true - y_pred) ** 2).sum()\n",
    "    ss_tot = ((y_true - y_true.mean()) ** 2).sum()\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae.item(),\n",
    "        \"mse\": mse.item(),\n",
    "        \"rmse\": rmse.item(),\n",
    "        \"r2\": r2.item(),\n",
    "        \"mean_error\": err.mean().item(),\n",
    "        \"std_error\": err.std().item()\n",
    "    }\n",
    "\n",
    "\n",
    "def percentage_within_threshold(y_true: torch.Tensor, y_pred: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "    \"\"\"Calculate percentage of predictions within threshold of true values.\"\"\"\n",
    "    y_true = y_true.detach().float().view(-1)\n",
    "    y_pred = y_pred.detach().float().view(-1)\n",
    "    \n",
    "    within_threshold = (torch.abs(y_pred - y_true) <= threshold).float().mean()\n",
    "    return within_threshold.item()\n",
    "\n",
    "print(\"âœ“ Regression metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_labels(y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert logits/probs to labels if needed.\n",
    "    If y_pred is (N, C) -> argmax to (N,)\n",
    "    If y_pred is (N,) -> assume already labels\n",
    "    \"\"\"\n",
    "    if y_pred.dim() == 2:\n",
    "        return torch.argmax(y_pred, dim=1)\n",
    "    return y_pred.view(-1)\n",
    "\n",
    "\n",
    "def accuracy(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Accuracy for classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (N,) long\n",
    "        y_pred: Logits (N, C) or labels (N,)\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy as float\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().view(-1).long()\n",
    "    y_hat = _to_labels(y_pred.detach()).view(-1).long()\n",
    "    return (y_hat == y_true).float().mean().item()\n",
    "\n",
    "\n",
    "def macro_f1(y_true: torch.Tensor, y_pred: torch.Tensor, num_classes: int = None, eps: float = 1e-9) -> float:\n",
    "    \"\"\"\n",
    "    Macro F1 score without sklearn dependency.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (N,) long\n",
    "        y_pred: Logits (N, C) or labels (N,)\n",
    "        num_classes: Number of classes (auto-detected if None)\n",
    "        eps: Small epsilon for numerical stability\n",
    "        \n",
    "    Returns:\n",
    "        Macro F1 score as float\n",
    "    \"\"\"\n",
    "    y_true = y_true.detach().view(-1).long()\n",
    "    y_hat = _to_labels(y_pred.detach()).view(-1).long()\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = int(torch.max(torch.cat([y_true, y_hat])).item()) + 1\n",
    "\n",
    "    f1_sum = 0.0\n",
    "    for c in range(num_classes):\n",
    "        tp = ((y_hat == c) & (y_true == c)).sum().float()\n",
    "        fp = ((y_hat == c) & (y_true != c)).sum().float()\n",
    "        fn = ((y_hat != c) & (y_true == c)).sum().float()\n",
    "\n",
    "        precision = tp / (tp + fp + eps)\n",
    "        recall = tp / (tp + fn + eps)\n",
    "        f1 = 2.0 * precision * recall / (precision + recall + eps)\n",
    "        f1_sum += f1.item()\n",
    "\n",
    "    return f1_sum / float(num_classes)\n",
    "\n",
    "\n",
    "def per_class_metrics(y_true: torch.Tensor, y_pred: torch.Tensor, class_names: List[str] = None) -> Dict:\n",
    "    \"\"\"Calculate per-class precision, recall, and F1 scores.\"\"\"\n",
    "    y_true = y_true.detach().view(-1).long()\n",
    "    y_hat = _to_labels(y_pred.detach()).view(-1).long()\n",
    "    \n",
    "    num_classes = int(torch.max(torch.cat([y_true, y_hat])).item()) + 1\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class_{i}\" for i in range(num_classes)]\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        tp = ((y_hat == c) & (y_true == c)).sum().float()\n",
    "        fp = ((y_hat == c) & (y_true != c)).sum().float()\n",
    "        fn = ((y_hat != c) & (y_true == c)).sum().float()\n",
    "        tn = ((y_hat != c) & (y_true != c)).sum().float()\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2.0 * precision * recall / (precision + recall + 1e-9)\n",
    "        \n",
    "        class_name = class_names[c] if c < len(class_names) else f\"Class_{c}\"\n",
    "        \n",
    "        metrics[class_name] = {\n",
    "            \"precision\": precision.item(),\n",
    "            \"recall\": recall.item(),\n",
    "            \"f1\": f1.item(),\n",
    "            \"support\": int((y_true == c).sum().item())\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"âœ“ Classification metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\", figsize=(8, 6)):\n",
    "    \"\"\"Plot confusion matrix for classification results.\"\"\"\n",
    "    y_true_np = y_true.detach().cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred_np = _to_labels(y_pred).detach().cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "    \n",
    "    cm = confusion_matrix(y_true_np, y_pred_np)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_regression_results(y_true, y_pred, title=\"Height Prediction Results\", figsize=(12, 4)):\n",
    "    \"\"\"Plot regression results with scatter plot and metrics.\"\"\"\n",
    "    y_true_np = y_true.detach().cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred_np = y_pred.detach().cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "    \n",
    "    metrics = regression_metrics(torch.tensor(y_true_np), torch.tensor(y_pred_np))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_true_np, y_pred_np, alpha=0.6, s=50)\n",
    "    min_val = min(y_true_np.min(), y_pred_np.min())\n",
    "    max_val = max(y_true_np.max(), y_pred_np.max())\n",
    "    axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    axes[0].set_xlabel('True Height (m)')\n",
    "    axes[0].set_ylabel('Predicted Height (m)')\n",
    "    axes[0].set_title(f'Predictions vs Truth\\nRÂ² = {metrics[\"r2\"]:.3f}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution\n",
    "    errors = y_pred_np - y_true_np\n",
    "    axes[1].hist(errors, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Prediction Error (m)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Error Distribution\\nMAE = {metrics[\"mae\"]:.3f}m')\n",
    "    axes[1].axvline(0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error vs true values\n",
    "    axes[2].scatter(y_true_np, errors, alpha=0.6, s=50)\n",
    "    axes[2].set_xlabel('True Height (m)')\n",
    "    axes[2].set_ylabel('Prediction Error (m)')\n",
    "    axes[2].set_title(f'Error vs True Values\\nRMSE = {metrics[\"rmse\"]:.3f}m')\n",
    "    axes[2].axhline(0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_training_metrics(train_losses, val_losses, train_metrics, val_metrics, figsize=(15, 10)):\n",
    "    \"\"\"Plot comprehensive training curves.\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, val_losses, 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE curves\n",
    "    train_mae = [m['mae'] for m in train_metrics]\n",
    "    val_mae = [m['mae'] for m in val_metrics]\n",
    "    axes[0, 1].plot(epochs, train_mae, 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, val_mae, 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 1].set_title('Height MAE')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('MAE (meters)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wave type accuracy\n",
    "    train_wt_acc = [m.get('wave_type_acc', 0) for m in train_metrics]\n",
    "    val_wt_acc = [m.get('wave_type_acc', 0) for m in val_metrics]\n",
    "    axes[0, 2].plot(epochs, train_wt_acc, 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, val_wt_acc, 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 2].set_title('Wave Type Accuracy')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Accuracy')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Direction accuracy\n",
    "    train_dir_acc = [m.get('direction_acc', 0) for m in train_metrics]\n",
    "    val_dir_acc = [m.get('direction_acc', 0) for m in val_metrics]\n",
    "    axes[1, 0].plot(epochs, train_dir_acc, 'b-', label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, val_dir_acc, 'r-', label='Validation', linewidth=2)\n",
    "    axes[1, 0].set_title('Direction Accuracy')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RÂ² score\n",
    "    train_r2 = [m.get('r2', 0) for m in train_metrics]\n",
    "    val_r2 = [m.get('r2', 0) for m in val_metrics]\n",
    "    axes[1, 1].plot(epochs, train_r2, 'b-', label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(epochs, val_r2, 'r-', label='Validation', linewidth=2)\n",
    "    axes[1, 1].set_title('RÂ² Score')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('RÂ²')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined F1 scores\n",
    "    train_wt_f1 = [m.get('wave_type_macro_f1', 0) for m in train_metrics]\n",
    "    val_wt_f1 = [m.get('wave_type_macro_f1', 0) for m in val_metrics]\n",
    "    train_dir_f1 = [m.get('direction_macro_f1', 0) for m in train_metrics]\n",
    "    val_dir_f1 = [m.get('direction_macro_f1', 0) for m in val_metrics]\n",
    "    \n",
    "    axes[1, 2].plot(epochs, train_wt_f1, 'b-', label='Train WT F1', linewidth=2)\n",
    "    axes[1, 2].plot(epochs, val_wt_f1, 'r-', label='Val WT F1', linewidth=2)\n",
    "    axes[1, 2].plot(epochs, train_dir_f1, 'b--', label='Train Dir F1', linewidth=2)\n",
    "    axes[1, 2].plot(epochs, val_dir_f1, 'r--', label='Val Dir F1', linewidth=2)\n",
    "    axes[1, 2].set_title('Macro F1 Scores')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('F1 Score')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for testing\n",
    "def create_dummy_predictions(batch_size=32, num_wave_types=5, num_directions=3):\n",
    "    \"\"\"Create dummy predictions for testing metrics.\"\"\"\n",
    "    \n",
    "    # Create realistic predictions\n",
    "    pred_h = torch.randn(batch_size, 1) * 0.5 + 1.5  # Heights around 1.5m\n",
    "    pred_wt = torch.randn(batch_size, num_wave_types)\n",
    "    pred_dir = torch.randn(batch_size, num_directions)\n",
    "\n",
    "    # Create corresponding targets\n",
    "    y_h = pred_h + torch.randn(batch_size, 1) * 0.3  # Add some noise\n",
    "    y_wt = torch.randint(0, num_wave_types, (batch_size,))\n",
    "    y_dir = torch.randint(0, num_directions, (batch_size,))\n",
    "    \n",
    "    # Sample weights (confidence)\n",
    "    sample_w = torch.rand(batch_size, 1) * 0.5 + 0.5  # Between 0.5 and 1.0\n",
    "    \n",
    "    return pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, sample_w\n",
    "\n",
    "# Generate test data\n",
    "print(\"Creating dummy data for testing...\")\n",
    "pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, sample_w = create_dummy_predictions()\n",
    "\n",
    "print(f\"âœ“ Created dummy data:\")\n",
    "print(f\"  Batch size: {pred_h.shape[0]}\")\n",
    "print(f\"  Height predictions shape: {pred_h.shape}\")\n",
    "print(f\"  Wave type predictions shape: {pred_wt.shape}\")\n",
    "print(f\"  Direction predictions shape: {pred_dir.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss function\n",
    "print(\"Testing multi-task loss function...\")\n",
    "\n",
    "loss_fn = MultiTaskLoss(w_height=1.0, w_wave_type=1.0, w_direction=1.0)\n",
    "total_loss, loss_dict = loss_fn(pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, sample_w)\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Results:\")\n",
    "print(f\"Total loss: {total_loss.item():.4f}\")\n",
    "print(f\"Height loss: {loss_dict['loss_h']:.4f}\")\n",
    "print(f\"Wave type loss: {loss_dict['loss_wt']:.4f}\")\n",
    "print(f\"Direction loss: {loss_dict['loss_dir']:.4f}\")\n",
    "\n",
    "# Test with different weights\n",
    "print(\"\\nTesting with different task weights...\")\n",
    "loss_fn_weighted = MultiTaskLoss(w_height=2.0, w_wave_type=1.0, w_direction=0.5)\n",
    "total_loss_w, loss_dict_w = loss_fn_weighted(pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, sample_w)\n",
    "\n",
    "print(f\"Weighted total loss: {total_loss_w.item():.4f}\")\n",
    "print(f\"Height contribution: {loss_dict_w['loss_h'] * 2.0:.4f}\")\n",
    "print(f\"Wave type contribution: {loss_dict_w['loss_wt'] * 1.0:.4f}\")\n",
    "print(f\"Direction contribution: {loss_dict_w['loss_dir'] * 0.5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test regression metrics\n",
    "print(\"Testing regression metrics...\")\n",
    "\n",
    "reg_metrics = regression_metrics(y_h, pred_h)\n",
    "print(f\"\\nðŸ“ Regression Metrics:\")\n",
    "for metric, value in reg_metrics.items():\n",
    "    print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "\n",
    "# Test threshold accuracy\n",
    "within_05 = percentage_within_threshold(y_h, pred_h, threshold=0.5)\n",
    "within_03 = percentage_within_threshold(y_h, pred_h, threshold=0.3)\n",
    "print(f\"\\nðŸŽ¯ Threshold Accuracy:\")\n",
    "print(f\"  Within 0.5m: {within_05:.1%}\")\n",
    "print(f\"  Within 0.3m: {within_03:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification metrics\n",
    "print(\"Testing classification metrics...\")\n",
    "\n",
    "wt_acc = accuracy(y_wt, pred_wt)\n",
    "dir_acc = accuracy(y_dir, pred_dir)\n",
    "wt_f1 = macro_f1(y_wt, pred_wt, pred_wt.shape[1])\n",
    "dir_f1 = macro_f1(y_dir, pred_dir, pred_dir.shape[1])\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Classification Metrics:\")\n",
    "print(f\"Wave type accuracy: {wt_acc:.4f}\")\n",
    "print(f\"Direction accuracy: {dir_acc:.4f}\")\n",
    "print(f\"Wave type macro F1: {wt_f1:.4f}\")\n",
    "print(f\"Direction macro F1: {dir_f1:.4f}\")\n",
    "\n",
    "# Test per-class metrics\n",
    "wave_types = [\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]\n",
    "directions = [\"left\", \"right\", \"both\"]\n",
    "\n",
    "wt_per_class = per_class_metrics(y_wt, pred_wt, wave_types)\n",
    "dir_per_class = per_class_metrics(y_dir, pred_dir, directions)\n",
    "\n",
    "print(f\"\\nðŸ“Š Per-Class Wave Type Metrics:\")\n",
    "for class_name, metrics in wt_per_class.items():\n",
    "    print(f\"  {class_name}: F1={metrics['f1']:.3f}, Support={metrics['support']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Per-Class Direction Metrics:\")\n",
    "for class_name, metrics in dir_per_class.items():\n",
    "    print(f\"  {class_name}: F1={metrics['f1']:.3f}, Support={metrics['support']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "print(\"Visualizing regression results...\")\n",
    "reg_metrics_vis = plot_regression_results(y_h, pred_h, \"Example Height Predictions\")\n",
    "\n",
    "print(f\"\\nVisualization metrics:\")\n",
    "for metric, value in reg_metrics_vis.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "print(\"Visualizing confusion matrices...\")\n",
    "\n",
    "# Wave type confusion matrix\n",
    "plot_confusion_matrix(y_wt, pred_wt, wave_types, \"Wave Type Predictions\")\n",
    "\n",
    "# Direction confusion matrix\n",
    "plot_confusion_matrix(y_dir, pred_dir, directions, \"Direction Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy training history for visualization\n",
    "print(\"Creating example training curves...\")\n",
    "\n",
    "# Simulate training history\n",
    "epochs = 20\n",
    "train_losses = [2.5 - 0.1*i + 0.05*np.random.randn() for i in range(epochs)]\n",
    "val_losses = [2.7 - 0.08*i + 0.08*np.random.randn() for i in range(epochs)]\n",
    "\n",
    "train_metrics = []\n",
    "val_metrics = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Simulate improving metrics\n",
    "    train_metrics.append({\n",
    "        'mae': 0.8 - 0.02*i + 0.01*np.random.randn(),\n",
    "        'r2': 0.1 + 0.03*i + 0.01*np.random.randn(),\n",
    "        'wave_type_acc': 0.3 + 0.02*i + 0.01*np.random.randn(),\n",
    "        'direction_acc': 0.4 + 0.015*i + 0.01*np.random.randn(),\n",
    "        'wave_type_macro_f1': 0.25 + 0.02*i + 0.01*np.random.randn(),\n",
    "        'direction_macro_f1': 0.35 + 0.015*i + 0.01*np.random.randn()\n",
    "    })\n",
    "    \n",
    "    val_metrics.append({\n",
    "        'mae': 0.85 - 0.015*i + 0.015*np.random.randn(),\n",
    "        'r2': 0.05 + 0.025*i + 0.015*np.random.randn(),\n",
    "        'wave_type_acc': 0.25 + 0.018*i + 0.015*np.random.randn(),\n",
    "        'direction_acc': 0.35 + 0.012*i + 0.015*np.random.randn(),\n",
    "        'wave_type_macro_f1': 0.2 + 0.018*i + 0.015*np.random.randn(),\n",
    "        'direction_macro_f1': 0.3 + 0.012*i + 0.015*np.random.randn()\n",
    "    })\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_metrics(train_losses, val_losses, train_metrics, val_metrics)\n",
    "\n",
    "print(\"âœ“ Training curves visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_comprehensive(pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, \n",
    "                               wave_type_names=None, direction_names=None, \n",
    "                               visualize=True):\n",
    "    \"\"\"Comprehensive model evaluation with all metrics and visualizations.\"\"\"\n",
    "    \n",
    "    print(\"Comprehensive Model Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Default class names\n",
    "    if wave_type_names is None:\n",
    "        wave_type_names = [\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]\n",
    "    if direction_names is None:\n",
    "        direction_names = [\"left\", \"right\", \"both\"]\n",
    "    \n",
    "    # Regression metrics\n",
    "    reg_metrics = regression_metrics(y_h, pred_h)\n",
    "    within_05 = percentage_within_threshold(y_h, pred_h, 0.5)\n",
    "    within_03 = percentage_within_threshold(y_h, pred_h, 0.3)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Height Regression Results:\")\n",
    "    print(f\"  MAE: {reg_metrics['mae']:.4f}m\")\n",
    "    print(f\"  RMSE: {reg_metrics['rmse']:.4f}m\")\n",
    "    print(f\"  RÂ²: {reg_metrics['r2']:.4f}\")\n",
    "    print(f\"  Within 0.5m: {within_05:.1%}\")\n",
    "    print(f\"  Within 0.3m: {within_03:.1%}\")\n",
    "    \n",
    "    # Classification metrics\n",
    "    wt_acc = accuracy(y_wt, pred_wt)\n",
    "    dir_acc = accuracy(y_dir, pred_dir)\n",
    "    wt_f1 = macro_f1(y_wt, pred_wt, pred_wt.shape[1])\n",
    "    dir_f1 = macro_f1(y_dir, pred_dir, pred_dir.shape[1])\n",
    "    \n",
    "    print(f\"\\nðŸŒŠ Wave Type Classification:\")\n",
    "    print(f\"  Accuracy: {wt_acc:.4f}\")\n",
    "    print(f\"  Macro F1: {wt_f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ§­ Direction Classification:\")\n",
    "    print(f\"  Accuracy: {dir_acc:.4f}\")\n",
    "    print(f\"  Macro F1: {dir_f1:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    wt_per_class = per_class_metrics(y_wt, pred_wt, wave_type_names)\n",
    "    dir_per_class = per_class_metrics(y_dir, pred_dir, direction_names)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Detailed Wave Type Performance:\")\n",
    "    for class_name, metrics in wt_per_class.items():\n",
    "        print(f\"  {class_name:12}: P={metrics['precision']:.3f} R={metrics['recall']:.3f} F1={metrics['f1']:.3f} (n={metrics['support']})\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Detailed Direction Performance:\")\n",
    "    for class_name, metrics in dir_per_class.items():\n",
    "        print(f\"  {class_name:12}: P={metrics['precision']:.3f} R={metrics['recall']:.3f} F1={metrics['f1']:.3f} (n={metrics['support']})\")\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = (reg_metrics['r2'] + wt_f1 + dir_f1) / 3\n",
    "    print(f\"\\nðŸŽ¯ Overall Performance Score: {overall_score:.4f}\")\n",
    "    print(f\"   (Average of RÂ², Wave Type F1, Direction F1)\")\n",
    "    \n",
    "    # Visualizations\n",
    "    if visualize:\n",
    "        print(f\"\\nðŸ“ˆ Generating visualizations...\")\n",
    "        plot_regression_results(y_h, pred_h, \"Model Height Predictions\")\n",
    "        plot_confusion_matrix(y_wt, pred_wt, wave_type_names, \"Wave Type Confusion Matrix\")\n",
    "        plot_confusion_matrix(y_dir, pred_dir, direction_names, \"Direction Confusion Matrix\")\n",
    "    \n",
    "    # Return summary\n",
    "    return {\n",
    "        'regression': reg_metrics,\n",
    "        'classification': {\n",
    "            'wave_type_acc': wt_acc,\n",
    "            'direction_acc': dir_acc,\n",
    "            'wave_type_f1': wt_f1,\n",
    "            'direction_f1': dir_f1\n",
    "        },\n",
    "        'per_class': {\n",
    "            'wave_type': wt_per_class,\n",
    "            'direction': dir_per_class\n",
    "        },\n",
    "        'overall_score': overall_score,\n",
    "        'threshold_accuracy': {\n",
    "            'within_0.5m': within_05,\n",
    "            'within_0.3m': within_03\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"Running comprehensive evaluation on dummy data...\")\n",
    "evaluation_results = evaluate_model_comprehensive(\n",
    "    pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir,\n",
    "    wave_type_names=[\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"],\n",
    "    direction_names=[\"left\", \"right\", \"both\"],\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Comprehensive evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete, standalone loss and metrics system for SwellSight:\n",
    "\n",
    "### âœ… **What we accomplished:**\n",
    "1. **Multi-Task Loss Function**: Combines regression and classification losses with task weighting\n",
    "2. **Comprehensive Metrics**: Regression (MAE, RMSE, RÂ²) and classification (accuracy, F1) metrics\n",
    "3. **Advanced Visualizations**: Training curves, confusion matrices, regression plots\n",
    "4. **Per-Class Analysis**: Detailed precision, recall, F1 for each class\n",
    "5. **Evaluation Framework**: Complete model assessment with visualizations\n",
    "6. **Threshold Analysis**: Percentage of predictions within error thresholds\n",
    "\n",
    "### ðŸ”§ **Key Features:**\n",
    "- **Sample Weighting**: Incorporates confidence scores into loss computation\n",
    "- **Task Balancing**: Adjustable weights for different tasks (height, type, direction)\n",
    "- **Robust Metrics**: Handles edge cases and provides numerical stability\n",
    "- **Rich Visualizations**: Publication-ready plots and analysis\n",
    "- **Comprehensive Evaluation**: Single function for complete model assessment\n",
    "\n",
    "### ðŸ“Š **Metrics Included:**\n",
    "- **Regression**: MAE, MSE, RMSE, RÂ², mean/std error, threshold accuracy\n",
    "- **Classification**: Accuracy, macro F1, per-class precision/recall/F1\n",
    "- **Loss Components**: Individual task losses with weighting\n",
    "- **Training Monitoring**: Multi-metric training curve visualization\n",
    "\n",
    "### ðŸš€ **Usage:**\n",
    "- Use `MultiTaskLoss` for training with sample weighting\n",
    "- Use individual metric functions for evaluation\n",
    "- Use `evaluate_model_comprehensive` for complete assessment\n",
    "- Use visualization functions for analysis and reporting\n",
    "\n",
    "**This notebook is completely standalone and includes all necessary functionality for loss computation and model evaluation!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}