{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwellSight Data Loading and Utilities\n",
    "\n",
    "This notebook contains the data loading utilities, dataset classes, and helper functions for the SwellSight project.\n",
    "\n",
    "## Components\n",
    "- Utility functions (seed setting, file I/O)\n",
    "- Dataset class for loading wave images and labels\n",
    "- Vocabulary building for classification tasks\n",
    "- Data transforms for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read JSONL file and return list of records.\"\"\"\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "\n",
    "def write_jsonl(items: List[Dict[str, Any]], path: str) -> None:\n",
    "    \"\"\"Write list of records to JSONL file.\"\"\"\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in items:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transforms(train: bool, image_size: int = 224):\n",
    "    \"\"\"Build data transforms for training or inference.\"\"\"\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.10, hue=0.03),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_infer_transform(image_size: int):\n",
    "    \"\"\"Build transform for single image inference.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Vocabulary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_to_weight(conf: str) -> float:\n",
    "    \"\"\"Convert confidence string to sample weight.\"\"\"\n",
    "    conf = (conf or \"medium\").lower().strip()\n",
    "    if conf == \"high\":\n",
    "        return 1.0\n",
    "    if conf == \"medium\":\n",
    "        return 0.7\n",
    "    if conf == \"low\":\n",
    "        return 0.4\n",
    "    return 0.7\n",
    "\n",
    "\n",
    "def build_vocabs(items: List[Dict[str, Any]]) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"Build vocabularies for wave types and directions.\"\"\"\n",
    "    wave_types = sorted({x[\"wave_type\"] for x in items})\n",
    "    directions = sorted({x[\"direction\"] for x in items})\n",
    "    return {k: i for i, k in enumerate(wave_types)}, {k: i for i, k in enumerate(directions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwellSightDataset(Dataset):\n",
    "    \"\"\"Dataset class for SwellSight wave analysis.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        index_jsonl: str,\n",
    "        transform=None,\n",
    "        wave_type_to_id: Dict[str, int] = None,\n",
    "        direction_to_id: Dict[str, int] = None,\n",
    "    ):\n",
    "        self.items = read_jsonl(index_jsonl)\n",
    "        self.transform = transform\n",
    "\n",
    "        if wave_type_to_id is None or direction_to_id is None:\n",
    "            self.wave_type_to_id, self.direction_to_id = build_vocabs(self.items)\n",
    "        else:\n",
    "            self.wave_type_to_id = wave_type_to_id\n",
    "            self.direction_to_id = direction_to_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        r = self.items[idx]\n",
    "        img = Image.open(r[\"image_path\"]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        height = torch.tensor([float(r[\"height_meters\"])], dtype=torch.float32)\n",
    "        wave_type = torch.tensor(self.wave_type_to_id[r[\"wave_type\"]], dtype=torch.long)\n",
    "        direction = torch.tensor(self.direction_to_id[r[\"direction\"]], dtype=torch.long)\n",
    "        weight = torch.tensor([confidence_to_weight(r.get(\"confidence\", \"medium\"))], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"height\": height,\n",
    "            \"wave_type\": wave_type,\n",
    "            \"direction\": direction,\n",
    "            \"weight\": weight,\n",
    "            \"meta\": r\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Example of creating transforms\n",
    "train_transform = build_transforms(train=True, image_size=224)\n",
    "val_transform = build_transforms(train=False, image_size=224)\n",
    "\n",
    "print(\"Training transforms:\")\n",
    "print(train_transform)\n",
    "print(\"\\nValidation transforms:\")\n",
    "print(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a dataset (if data exists)\n",
    "# Uncomment and modify path when you have actual data\n",
    "\n",
    "# try:\n",
    "#     dataset = SwellSightDataset(\n",
    "#         \"data/processed/splits/train.jsonl\",\n",
    "#         transform=train_transform\n",
    "#     )\n",
    "#     print(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "#     print(f\"Wave types: {dataset.wave_type_to_id}\")\n",
    "#     print(f\"Directions: {dataset.direction_to_id}\")\n",
    "#     \n",
    "#     # Show first sample\n",
    "#     sample = dataset[0]\n",
    "#     print(f\"\\nFirst sample:\")\n",
    "#     print(f\"Image shape: {sample['image'].shape}\")\n",
    "#     print(f\"Height: {sample['height'].item():.2f}m\")\n",
    "#     print(f\"Wave type ID: {sample['wave_type'].item()}\")\n",
    "#     print(f\"Direction ID: {sample['direction'].item()}\")\n",
    "#     print(f\"Weight: {sample['weight'].item():.2f}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Dataset file not found. Run data preparation notebooks first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}