{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwellSight Model Training\n",
    "\n",
    "This notebook trains the SwellSight multi-task model for wave analysis.\n",
    "\n",
    "## Purpose\n",
    "- Train the model on mixed real and synthetic data\n",
    "- Monitor training progress with live metrics\n",
    "- Implement early stopping and checkpointing\n",
    "- Visualize training curves and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import components from previous notebooks\n",
    "%run 01_model_architecture.ipynb\n",
    "%run 02_data_loading.ipynb\n",
    "%run 03_loss_and_metrics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    \"train_jsonl\": \"data/synthetic/mix/train.jsonl\",\n",
    "    \"val_jsonl\": \"data/synthetic/mix/val.jsonl\",\n",
    "    \"out_dir\": f\"runs/swell_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    \n",
    "    # Model parameters\n",
    "    \"image_size\": 224,\n",
    "    \"dropout\": 0.35,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 25,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \n",
    "    # Loss weights\n",
    "    \"w_height\": 1.0,\n",
    "    \"w_wave_type\": 1.0,\n",
    "    \"w_direction\": 1.0,\n",
    "    \n",
    "    # Other\n",
    "    \"seed\": 42,\n",
    "    \"num_workers\": 2,\n",
    "    \"pin_memory\": True,\n",
    "    \"patience\": 5,  # Early stopping patience\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed and device\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "ensure_dir(CONFIG[\"out_dir\"])\n",
    "print(f\"Output directory: {CONFIG['out_dir']}\")\n",
    "\n",
    "# Save configuration\n",
    "with open(os.path.join(CONFIG[\"out_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and build vocabularies\n",
    "try:\n",
    "    train_items = read_jsonl(CONFIG[\"train_jsonl\"])\n",
    "    print(f\"✓ Loaded {len(train_items)} training samples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ Training data not found: {CONFIG['train_jsonl']}\")\n",
    "    print(\"Please run the data preparation notebooks first.\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"Creating dummy training data...\")\n",
    "    train_items = [\n",
    "        {\n",
    "            \"image_path\": f\"dummy_train_{i}.jpg\",\n",
    "            \"height_meters\": float(np.random.uniform(0.5, 2.5)),\n",
    "            \"wave_type\": np.random.choice([\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]),\n",
    "            \"direction\": np.random.choice([\"left\", \"right\", \"both\"]),\n",
    "            \"confidence\": \"high\",\n",
    "            \"source\": \"dummy\"\n",
    "        }\n",
    "        for i in range(200)\n",
    "    ]\n",
    "\n",
    "try:\n",
    "    val_items = read_jsonl(CONFIG[\"val_jsonl\"])\n",
    "    print(f\"✓ Loaded {len(val_items)} validation samples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ Validation data not found: {CONFIG['val_jsonl']}\")\n",
    "    # Create dummy validation data\n",
    "    val_items = [\n",
    "        {\n",
    "            \"image_path\": f\"dummy_val_{i}.jpg\",\n",
    "            \"height_meters\": float(np.random.uniform(0.5, 2.5)),\n",
    "            \"wave_type\": np.random.choice([\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]),\n",
    "            \"direction\": np.random.choice([\"left\", \"right\", \"both\"]),\n",
    "            \"confidence\": \"high\",\n",
    "            \"source\": \"dummy\"\n",
    "        }\n",
    "        for i in range(50)\n",
    "    ]\n",
    "\n",
    "# Build vocabularies from training data\n",
    "wt2id, d2id = build_vocabs(train_items)\n",
    "print(f\"\\nVocabularies:\")\n",
    "print(f\"Wave types: {wt2id}\")\n",
    "print(f\"Directions: {d2id}\")\n",
    "\n",
    "# Save vocabularies\n",
    "vocabs = {\"wave_type_to_id\": wt2id, \"direction_to_id\": d2id}\n",
    "with open(os.path.join(CONFIG[\"out_dir\"], \"vocabs.json\"), \"w\") as f:\n",
    "    json.dump(vocabs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "# Note: For demonstration with dummy data, we'll create a simple dataset\n",
    "# In practice, you would use the actual SwellSightDataset\n",
    "\n",
    "class DummyDataset:\n",
    "    \"\"\"Dummy dataset for demonstration when real images are not available.\"\"\"\n",
    "    def __init__(self, items, transform, wave_type_to_id, direction_to_id):\n",
    "        self.items = items\n",
    "        self.transform = transform\n",
    "        self.wave_type_to_id = wave_type_to_id\n",
    "        self.direction_to_id = direction_to_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r = self.items[idx]\n",
    "        \n",
    "        # Create dummy image tensor\n",
    "        if self.transform:\n",
    "            # Simulate transformed image\n",
    "            img = torch.randn(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])\n",
    "        else:\n",
    "            img = torch.randn(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])\n",
    "        \n",
    "        height = torch.tensor([float(r[\"height_meters\"])], dtype=torch.float32)\n",
    "        wave_type = torch.tensor(self.wave_type_to_id[r[\"wave_type\"]], dtype=torch.long)\n",
    "        direction = torch.tensor(self.direction_to_id[r[\"direction\"]], dtype=torch.long)\n",
    "        weight = torch.tensor([1.0], dtype=torch.float32)  # High confidence\n",
    "        \n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"height\": height,\n",
    "            \"wave_type\": wave_type,\n",
    "            \"direction\": direction,\n",
    "            \"weight\": weight,\n",
    "            \"meta\": r\n",
    "        }\n",
    "\n",
    "# Check if we have real images or use dummy data\n",
    "use_dummy_data = not os.path.exists(train_items[0][\"image_path\"]) if train_items else True\n",
    "\n",
    "if use_dummy_data:\n",
    "    print(\"⚠️ Using dummy data for demonstration (no real images found)\")\n",
    "    train_ds = DummyDataset(\n",
    "        train_items,\n",
    "        transform=build_transforms(True, CONFIG[\"image_size\"]),\n",
    "        wave_type_to_id=wt2id,\n",
    "        direction_to_id=d2id\n",
    "    )\n",
    "    val_ds = DummyDataset(\n",
    "        val_items,\n",
    "        transform=build_transforms(False, CONFIG[\"image_size\"]),\n",
    "        wave_type_to_id=wt2id,\n",
    "        direction_to_id=d2id\n",
    "    )\n",
    "else:\n",
    "    print(\"✓ Using real image data\")\n",
    "    train_ds = SwellSightDataset(\n",
    "        CONFIG[\"train_jsonl\"],\n",
    "        transform=build_transforms(True, CONFIG[\"image_size\"]),\n",
    "        wave_type_to_id=wt2id,\n",
    "        direction_to_id=d2id\n",
    "    )\n",
    "    val_ds = SwellSightDataset(\n",
    "        CONFIG[\"val_jsonl\"],\n",
    "        transform=build_transforms(False, CONFIG[\"image_size\"]),\n",
    "        wave_type_to_id=wt2id,\n",
    "        direction_to_id=d2id\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=CONFIG[\"num_workers\"], \n",
    "    pin_memory=CONFIG[\"pin_memory\"]\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=CONFIG[\"num_workers\"], \n",
    "    pin_memory=CONFIG[\"pin_memory\"]\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SwellSightNet(\n",
    "    num_wave_types=len(wt2id), \n",
    "    num_directions=len(d2id), \n",
    "    dropout=CONFIG[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = MultiTaskLoss(\n",
    "    w_height=CONFIG[\"w_height\"],\n",
    "    w_wave_type=CONFIG[\"w_wave_type\"],\n",
    "    w_direction=CONFIG[\"w_direction\"]\n",
    ")\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG[\"lr\"], \n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode=\"min\", \n",
    "    factor=0.5, \n",
    "    patience=CONFIG[\"patience\"]//2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if device == \"cuda\" else None\n",
    "\n",
    "print(\"✓ Model, optimizer, and scheduler created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer, loss_fn, device, train: bool, scaler=None):\n",
    "    \"\"\"Run one epoch of training or validation.\"\"\"\n",
    "    model.train() if train else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    all_h_pred, all_h_true = [], []\n",
    "    all_wt_logits, all_wt_true = [], []\n",
    "    all_dir_logits, all_dir_true = [], []\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=f\"{'Train' if train else 'Val'} Epoch\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        x = batch[\"image\"].to(device)\n",
    "        y_h = batch[\"height\"].to(device)\n",
    "        y_wt = batch[\"wave_type\"].to(device).view(-1)\n",
    "        y_dir = batch[\"direction\"].to(device).view(-1)\n",
    "        w = batch[\"weight\"].to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            with autocast(enabled=(device == \"cuda\" and scaler is not None)):\n",
    "                pred_h, pred_wt, pred_dir = model(x)\n",
    "                loss, loss_dict = loss_fn(pred_h, pred_wt, pred_dir, y_h, y_wt, y_dir, w)\n",
    "\n",
    "            if train and scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            elif train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        n += x.size(0)\n",
    "\n",
    "        # Collect predictions for metrics\n",
    "        all_h_pred.append(pred_h.detach().cpu())\n",
    "        all_h_true.append(y_h.detach().cpu())\n",
    "        all_wt_logits.append(pred_wt.detach().cpu())\n",
    "        all_wt_true.append(y_wt.detach().cpu())\n",
    "        all_dir_logits.append(pred_dir.detach().cpu())\n",
    "        all_dir_true.append(y_dir.detach().cpu())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'h_loss': f'{loss_dict[\"loss_h\"]:.4f}',\n",
    "            'wt_loss': f'{loss_dict[\"loss_wt\"]:.4f}',\n",
    "            'dir_loss': f'{loss_dict[\"loss_dir\"]:.4f}'\n",
    "        })\n",
    "\n",
    "    # Compute metrics\n",
    "    h_pred = torch.cat(all_h_pred, 0)\n",
    "    h_true = torch.cat(all_h_true, 0)\n",
    "    wt_logits = torch.cat(all_wt_logits, 0)\n",
    "    wt_true = torch.cat(all_wt_true, 0)\n",
    "    dir_logits = torch.cat(all_dir_logits, 0)\n",
    "    dir_true = torch.cat(all_dir_true, 0)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics.update(regression_metrics(h_true, h_pred))\n",
    "    metrics[\"wave_type_acc\"] = accuracy(wt_true, wt_logits)\n",
    "    metrics[\"direction_acc\"] = accuracy(dir_true, dir_logits)\n",
    "    metrics[\"wave_type_macro_f1\"] = macro_f1(wt_true, wt_logits, wt_logits.shape[1])\n",
    "    metrics[\"direction_macro_f1\"] = macro_f1(dir_true, dir_logits, dir_logits.shape[1])\n",
    "\n",
    "    return (total_loss / max(1, n)), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_metrics\": [],\n",
    "    \"val_metrics\": [],\n",
    "    \"lr\": [],\n",
    "    \"epoch_times\": []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"Starting training for {CONFIG['epochs']} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['lr']}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_metrics = run_epoch(\n",
    "        model, train_loader, optimizer, loss_fn, device, train=True, scaler=scaler\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_metrics = run_epoch(\n",
    "        model, val_loader, optimizer, loss_fn, device, train=False, scaler=None\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Record history\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_metrics\"].append(train_metrics)\n",
    "    history[\"val_metrics\"].append(val_metrics)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    history[\"epoch_times\"].append(epoch_time)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "    print(f\"Train MAE: {train_metrics['mae']:.4f} | Val MAE: {val_metrics['mae']:.4f}\")\n",
    "    print(f\"Train WT Acc: {train_metrics['wave_type_acc']:.4f} | Val WT Acc: {val_metrics['wave_type_acc']:.4f}\")\n",
    "    print(f\"Train Dir Acc: {train_metrics['direction_acc']:.4f} | Val Dir Acc: {val_metrics['direction_acc']:.4f}\")\n",
    "    print(f\"Epoch time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"val_loss\": val_loss,\n",
    "        \"config\": CONFIG\n",
    "    }\n",
    "    \n",
    "    # Save last checkpoint\n",
    "    torch.save(checkpoint, os.path.join(CONFIG[\"out_dir\"], \"last.pt\"))\n",
    "    \n",
    "    # Save best checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        torch.save(checkpoint, os.path.join(CONFIG[\"out_dir\"], \"best.pt\"))\n",
    "        print(f\"✓ New best model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG[\"patience\"]:\n",
    "        print(f\"\\nEarly stopping triggered after {patience_counter} epochs without improvement\")\n",
    "        print(f\"Best model was at epoch {best_epoch} with val_loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "    \n",
    "    # Save history every few epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == CONFIG[\"epochs\"] - 1:\n",
    "        with open(os.path.join(CONFIG[\"out_dir\"], \"history.json\"), \"w\") as f:\n",
    "            # Convert numpy types to native Python types for JSON serialization\n",
    "            history_serializable = {}\n",
    "            for key, values in history.items():\n",
    "                if key in [\"train_metrics\", \"val_metrics\"]:\n",
    "                    history_serializable[key] = [\n",
    "                        {k: float(v) for k, v in metrics.items()} for metrics in values\n",
    "                    ]\n",
    "                else:\n",
    "                    history_serializable[key] = [float(v) for v in values]\n",
    "            json.dump(history_serializable, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best model: epoch {best_epoch}, val_loss: {best_val_loss:.4f}\")\n",
    "print(f\"Total training time: {sum(history['epoch_times']):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot comprehensive training curves.\"\"\"\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(epochs, history[\"train_loss\"], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history[\"val_loss\"], 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE curves\n",
    "    train_mae = [m['mae'] for m in history[\"train_metrics\"]]\n",
    "    val_mae = [m['mae'] for m in history[\"val_metrics\"]]\n",
    "    axes[0, 1].plot(epochs, train_mae, 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, val_mae, 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 1].set_title('Height MAE')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('MAE (meters)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wave type accuracy\n",
    "    train_wt_acc = [m['wave_type_acc'] for m in history[\"train_metrics\"]]\n",
    "    val_wt_acc = [m['wave_type_acc'] for m in history[\"val_metrics\"]]\n",
    "    axes[0, 2].plot(epochs, train_wt_acc, 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, val_wt_acc, 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 2].set_title('Wave Type Accuracy')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Accuracy')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Direction accuracy\n",
    "    train_dir_acc = [m['direction_acc'] for m in history[\"train_metrics\"]]\n",
    "    val_dir_acc = [m['direction_acc'] for m in history[\"val_metrics\"]]\n",
    "    axes[1, 0].plot(epochs, train_dir_acc, 'b-', label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, val_dir_acc, 'r-', label='Validation', linewidth=2)\n",
    "    axes[1, 0].set_title('Direction Accuracy')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    axes[1, 1].plot(epochs, history[\"lr\"], 'g-', linewidth=2)\n",
    "    axes[1, 1].set_title('Learning Rate')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Epoch times\n",
    "    axes[1, 2].plot(epochs, history[\"epoch_times\"], 'purple', linewidth=2)\n",
    "    axes[1, 2].set_title('Epoch Duration')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Time (seconds)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"out_dir\"], \"training_curves.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary statistics\n",
    "print(\"Training Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final learning rate: {history['lr'][-1]:.2e}\")\n",
    "print(f\"Total training time: {sum(history['epoch_times']):.1f}s\")\n",
    "print(f\"Average epoch time: {np.mean(history['epoch_times']):.1f}s\")\n",
    "\n",
    "# Best model metrics\n",
    "best_idx = best_epoch - 1\n",
    "if best_idx < len(history['val_metrics']):\n",
    "    best_metrics = history['val_metrics'][best_idx]\n",
    "    print(f\"\\nBest Model Performance:\")\n",
    "    print(f\"  Height MAE: {best_metrics['mae']:.4f}m\")\n",
    "    print(f\"  Height RMSE: {best_metrics['rmse']:.4f}m\")\n",
    "    print(f\"  Wave Type Accuracy: {best_metrics['wave_type_acc']:.4f}\")\n",
    "    print(f\"  Direction Accuracy: {best_metrics['direction_acc']:.4f}\")\n",
    "    print(f\"  Wave Type Macro F1: {best_metrics['wave_type_macro_f1']:.4f}\")\n",
    "    print(f\"  Direction Macro F1: {best_metrics['direction_macro_f1']:.4f}\")\n",
    "\n",
    "# Final model metrics\n",
    "final_metrics = history['val_metrics'][-1]\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"  Height MAE: {final_metrics['mae']:.4f}m\")\n",
    "print(f\"  Height RMSE: {final_metrics['rmse']:.4f}m\")\n",
    "print(f\"  Wave Type Accuracy: {final_metrics['wave_type_acc']:.4f}\")\n",
    "print(f\"  Direction Accuracy: {final_metrics['direction_acc']:.4f}\")\n",
    "print(f\"  Wave Type Macro F1: {final_metrics['wave_type_macro_f1']:.4f}\")\n",
    "print(f\"  Direction Macro F1: {final_metrics['direction_macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for analysis\n",
    "best_checkpoint = torch.load(os.path.join(CONFIG[\"out_dir\"], \"best.pt\"), map_location=device)\n",
    "model.load_state_dict(best_checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {best_checkpoint['epoch']}\")\n",
    "\n",
    "# Get predictions on validation set\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Getting predictions\"):\n",
    "        x = batch[\"image\"].to(device)\n",
    "        y_h = batch[\"height\"].cpu()\n",
    "        y_wt = batch[\"wave_type\"].cpu()\n",
    "        y_dir = batch[\"direction\"].cpu()\n",
    "        \n",
    "        pred_h, pred_wt, pred_dir = model(x)\n",
    "        \n",
    "        all_predictions.append({\n",
    "            \"height\": pred_h.cpu(),\n",
    "            \"wave_type\": pred_wt.cpu(),\n",
    "            \"direction\": pred_dir.cpu()\n",
    "        })\n",
    "        \n",
    "        all_targets.append({\n",
    "            \"height\": y_h,\n",
    "            \"wave_type\": y_wt,\n",
    "            \"direction\": y_dir\n",
    "        })\n",
    "\n",
    "# Concatenate all predictions and targets\n",
    "pred_heights = torch.cat([p[\"height\"] for p in all_predictions], 0)\n",
    "pred_wave_types = torch.cat([p[\"wave_type\"] for p in all_predictions], 0)\n",
    "pred_directions = torch.cat([p[\"direction\"] for p in all_predictions], 0)\n",
    "\n",
    "true_heights = torch.cat([t[\"height\"] for t in all_targets], 0)\n",
    "true_wave_types = torch.cat([t[\"wave_type\"] for t in all_targets], 0)\n",
    "true_directions = torch.cat([t[\"direction\"] for t in all_targets], 0)\n",
    "\n",
    "print(f\"Collected predictions for {len(pred_heights)} validation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs targets\n",
    "wave_type_names = list(wt2id.keys())\n",
    "direction_names = list(d2id.keys())\n",
    "\n",
    "# Height regression results\n",
    "plot_regression_results(true_heights, pred_heights, \"Validation Set - Height Predictions\")\n",
    "\n",
    "# Classification confusion matrices\n",
    "plot_confusion_matrix(true_wave_types, pred_wave_types, wave_type_names, \"Wave Type Predictions\")\n",
    "plot_confusion_matrix(true_directions, pred_directions, direction_names, \"Direction Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final training summary\n",
    "training_summary = {\n",
    "    \"config\": CONFIG,\n",
    "    \"total_epochs\": len(history['train_loss']),\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "    \"final_lr\": history['lr'][-1],\n",
    "    \"total_training_time\": sum(history['epoch_times']),\n",
    "    \"avg_epoch_time\": np.mean(history['epoch_times']),\n",
    "    \"best_metrics\": history['val_metrics'][best_epoch-1] if best_epoch <= len(history['val_metrics']) else None,\n",
    "    \"final_metrics\": history['val_metrics'][-1],\n",
    "    \"model_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "    \"vocabularies\": vocabs\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG[\"out_dir\"], \"training_summary.json\"), \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Training completed successfully!\")\n",
    "print(f\"✓ Model checkpoints saved to: {CONFIG['out_dir']}\")\n",
    "print(f\"✓ Best model: best.pt (epoch {best_epoch}, val_loss: {best_val_loss:.4f})\")\n",
    "print(f\"✓ Latest model: last.pt\")\n",
    "print(f\"✓ Training history: history.json\")\n",
    "print(f\"✓ Vocabularies: vocabs.json\")\n",
    "print(f\"✓ Configuration: config.json\")\n",
    "print(f\"✓ Summary: training_summary.json\")\n",
    "print(f\"✓ Training curves: training_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}