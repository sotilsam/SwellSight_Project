{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Real Dataset Index (Standalone)\n",
    "\n",
    "This notebook creates an index file for the real wave dataset from the labels.json file.\n",
    "**This is a standalone version that includes all necessary code and doesn't require external files.**\n",
    "\n",
    "## Purpose\n",
    "- Load wave parameter labels from JSON file\n",
    "- Verify image files exist on disk\n",
    "- Create JSONL index for downstream processing\n",
    "- Analyze dataset statistics\n",
    "- Generate dummy data if real data is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not available\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "required_packages = ['pandas', 'matplotlib', 'seaborn', 'numpy']\n",
    "for package in required_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "print(\"‚úì All required packages are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str) -> None:\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    if path:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_jsonl(items: List[Dict[str, Any]], path: str) -> None:\n",
    "    \"\"\"Write list of records to JSONL file.\"\"\"\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in items:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_json(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file with error handling.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {path}\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Warning: Invalid JSON in {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def save_json(data: Dict[str, Any], path: str) -> None:\n",
    "    \"\"\"Save data to JSON file.\"\"\"\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úì Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - modify these paths as needed\n",
    "CONFIG = {\n",
    "    \"images_dir\": \"data/real/images\",\n",
    "    \"labels_json\": \"data/real/labels.json\",\n",
    "    \"out_index\": \"data/processed/real_index.jsonl\",\n",
    "    \"create_dummy_if_missing\": True,  # Create dummy data if real data not found\n",
    "    \"dummy_samples\": 100  # Number of dummy samples to create\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_labels(n_samples: int = 100) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Create dummy labels for demonstration when real data is not available.\"\"\"\n",
    "    \n",
    "    wave_types = [\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]\n",
    "    directions = [\"left\", \"right\", \"both\"]\n",
    "    confidences = [\"high\", \"medium\", \"low\"]\n",
    "    \n",
    "    labels = {}\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        filename = f\"wave_image_{i:03d}.jpg\"\n",
    "        \n",
    "        # Create realistic wave parameters\n",
    "        height = float(np.random.uniform(0.3, 3.0))\n",
    "        wave_type = np.random.choice(wave_types)\n",
    "        direction = np.random.choice(directions)\n",
    "        confidence = np.random.choice(confidences, p=[0.5, 0.3, 0.2])  # More high confidence\n",
    "        \n",
    "        # Add some correlation between wave type and height\n",
    "        if wave_type == \"closeout\":\n",
    "            height = max(0.5, height * 0.7)  # Closeouts tend to be smaller\n",
    "        elif wave_type == \"reef_break\":\n",
    "            height = min(3.0, height * 1.3)  # Reef breaks can be bigger\n",
    "        \n",
    "        labels[filename] = {\n",
    "            \"height_meters\": round(height, 2),\n",
    "            \"wave_type\": wave_type,\n",
    "            \"direction\": direction,\n",
    "            \"confidence\": confidence,\n",
    "            \"notes\": f\"Dummy sample {i+1} - {wave_type} wave\",\n",
    "            \"data_key\": i + 1\n",
    "        }\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def create_dummy_images_info(labels: Dict[str, Dict[str, Any]], images_dir: str) -> List[str]:\n",
    "    \"\"\"Create dummy image file information.\"\"\"\n",
    "    \n",
    "    # Create the images directory\n",
    "    ensure_dir(images_dir)\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    for filename in labels.keys():\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "        \n",
    "        # Create a simple text file as placeholder (not a real image)\n",
    "        # In practice, you would have real image files here\n",
    "        if not os.path.exists(image_path):\n",
    "            with open(image_path + \".txt\", \"w\") as f:\n",
    "                f.write(f\"Dummy image placeholder for {filename}\\n\")\n",
    "                f.write(f\"Wave: {labels[filename]['wave_type']}\\n\")\n",
    "                f.write(f\"Height: {labels[filename]['height_meters']}m\\n\")\n",
    "                f.write(f\"Direction: {labels[filename]['direction']}\\n\")\n",
    "            \n",
    "            created_files.append(image_path + \".txt\")\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "print(\"‚úì Dummy data generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_real_index(images_dir: str, labels_json: str, out_jsonl: str, \n",
    "                    create_dummy: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Build index from real dataset labels and images.\n",
    "    \n",
    "    Args:\n",
    "        images_dir: Directory containing image files\n",
    "        labels_json: Path to labels JSON file\n",
    "        out_jsonl: Output JSONL file path\n",
    "        create_dummy: Whether to create dummy data if real data is missing\n",
    "        \n",
    "    Returns:\n",
    "        List of dataset records\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Building real dataset index...\")\n",
    "    print(f\"Images directory: {images_dir}\")\n",
    "    print(f\"Labels file: {labels_json}\")\n",
    "    print(f\"Output file: {out_jsonl}\")\n",
    "    \n",
    "    # Try to load real labels\n",
    "    labels = load_json(labels_json)\n",
    "    \n",
    "    if not labels and create_dummy:\n",
    "        print(f\"\\n‚ö†Ô∏è Real labels not found. Creating dummy data for demonstration...\")\n",
    "        labels = create_dummy_labels(CONFIG[\"dummy_samples\"])\n",
    "        \n",
    "        # Save dummy labels for reference\n",
    "        ensure_dir(os.path.dirname(labels_json))\n",
    "        save_json(labels, labels_json)\n",
    "        print(f\"‚úì Created dummy labels file: {labels_json}\")\n",
    "        \n",
    "        # Create dummy image placeholders\n",
    "        created_files = create_dummy_images_info(labels, images_dir)\n",
    "        print(f\"‚úì Created {len(created_files)} dummy image placeholders\")\n",
    "    \n",
    "    if not isinstance(labels, dict):\n",
    "        raise ValueError(\"labels.json must be a dict: {filename: {...}}\")\n",
    "\n",
    "    ensure_dir(os.path.dirname(out_jsonl))\n",
    "\n",
    "    records = []\n",
    "    missing = 0\n",
    "    found = 0\n",
    "\n",
    "    print(f\"\\nProcessing {len(labels)} labeled samples...\")\n",
    "    \n",
    "    for filename, ann in labels.items():\n",
    "        # Check for actual image file or placeholder\n",
    "        img_path = os.path.join(images_dir, filename)\n",
    "        img_path_txt = img_path + \".txt\"  # Dummy placeholder\n",
    "        \n",
    "        # Accept either real image or placeholder\n",
    "        if os.path.exists(img_path):\n",
    "            final_path = img_path\n",
    "            found += 1\n",
    "        elif os.path.exists(img_path_txt):\n",
    "            final_path = img_path  # Still use original name for consistency\n",
    "            found += 1\n",
    "        else:\n",
    "            missing += 1\n",
    "            print(f\"Warning: Missing image {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Validate required fields\n",
    "        try:\n",
    "            height_meters = float(ann[\"height_meters\"])\n",
    "            wave_type = str(ann[\"wave_type\"])\n",
    "            direction = str(ann[\"direction\"])\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Warning: Invalid annotation for {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        rec = {\n",
    "            \"image_path\": final_path,\n",
    "            \"height_meters\": height_meters,\n",
    "            \"wave_type\": wave_type,\n",
    "            \"direction\": direction,\n",
    "            \"confidence\": str(ann.get(\"confidence\", \"medium\")),\n",
    "            \"notes\": str(ann.get(\"notes\", \"\")),\n",
    "            \"data_key\": int(ann.get(\"data_key\", -1)),\n",
    "            \"source\": \"real\",\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    # Write JSONL\n",
    "    write_jsonl(records, out_jsonl)\n",
    "\n",
    "    print(f\"\\n‚úÖ Index building complete!\")\n",
    "    print(f\"‚úì Saved {len(records)} records to {out_jsonl}\")\n",
    "    print(f\"‚úì Found {found} images\")\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è {missing} images missing on disk\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "print(\"‚úì Index building function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input files exist\n",
    "print(\"Checking for existing data files...\")\n",
    "\n",
    "labels_exist = os.path.exists(CONFIG[\"labels_json\"])\n",
    "images_exist = os.path.exists(CONFIG[\"images_dir\"])\n",
    "\n",
    "if labels_exist:\n",
    "    print(f\"‚úì Labels file found: {CONFIG['labels_json']}\")\n",
    "else:\n",
    "    print(f\"‚úó Labels file not found: {CONFIG['labels_json']}\")\n",
    "\n",
    "if images_exist:\n",
    "    print(f\"‚úì Images directory found: {CONFIG['images_dir']}\")\n",
    "    try:\n",
    "        image_files = [f for f in os.listdir(CONFIG[\"images_dir\"]) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.txt'))]\n",
    "        print(f\"  Found {len(image_files)} files\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not list files: {e}\")\n",
    "else:\n",
    "    print(f\"‚úó Images directory not found: {CONFIG['images_dir']}\")\n",
    "\n",
    "if not labels_exist or not images_exist:\n",
    "    if CONFIG[\"create_dummy_if_missing\"]:\n",
    "        print(f\"\\nüîÑ Will create dummy data for demonstration\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Real data files missing and dummy creation disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the index\n",
    "records = build_real_index(\n",
    "    CONFIG[\"images_dir\"], \n",
    "    CONFIG[\"labels_json\"], \n",
    "    CONFIG[\"out_index\"],\n",
    "    create_dummy=CONFIG[\"create_dummy_if_missing\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Index built with {len(records)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "if records:\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    print(f\"Dataset Summary:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    print(f\"\\nüìè Height statistics:\")\n",
    "    height_stats = df['height_meters'].describe()\n",
    "    for stat, value in height_stats.items():\n",
    "        print(f\"  {stat}: {value:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüåä Wave type distribution:\")\n",
    "    wave_type_counts = df['wave_type'].value_counts()\n",
    "    for wt, count in wave_type_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {wt}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüß≠ Direction distribution:\")\n",
    "    direction_counts = df['direction'].value_counts()\n",
    "    for direction, count in direction_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {direction}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Confidence distribution:\")\n",
    "    confidence_counts = df['confidence'].value_counts()\n",
    "    for conf, count in confidence_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {conf}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"No records to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset statistics\n",
    "if records and len(records) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Height distribution\n",
    "    axes[0, 0].hist(df['height_meters'], bins=20, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "    axes[0, 0].set_title('Wave Height Distribution')\n",
    "    axes[0, 0].set_xlabel('Height (meters)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(df['height_meters'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {df[\"height_meters\"].mean():.2f}m')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wave type distribution\n",
    "    wave_type_counts = df['wave_type'].value_counts()\n",
    "    bars1 = axes[0, 1].bar(wave_type_counts.index, wave_type_counts.values, \n",
    "                          alpha=0.7, color='lightcoral')\n",
    "    axes[0, 1].set_title('Wave Type Distribution')\n",
    "    axes[0, 1].set_xlabel('Wave Type')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars1, wave_type_counts.values):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                        str(count), ha='center', va='bottom')\n",
    "    \n",
    "    # Direction distribution (pie chart)\n",
    "    direction_counts = df['direction'].value_counts()\n",
    "    colors = ['lightgreen', 'orange', 'lightblue']\n",
    "    axes[1, 0].pie(direction_counts.values, labels=direction_counts.index, \n",
    "                   autopct='%1.1f%%', colors=colors[:len(direction_counts)])\n",
    "    axes[1, 0].set_title('Direction Distribution')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    confidence_counts = df['confidence'].value_counts()\n",
    "    conf_colors = {'high': 'green', 'medium': 'orange', 'low': 'red'}\n",
    "    bar_colors = [conf_colors.get(conf, 'gray') for conf in confidence_counts.index]\n",
    "    \n",
    "    bars2 = axes[1, 1].bar(confidence_counts.index, confidence_counts.values, \n",
    "                          alpha=0.7, color=bar_colors)\n",
    "    axes[1, 1].set_title('Confidence Distribution')\n",
    "    axes[1, 1].set_xlabel('Confidence Level')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars2, confidence_counts.values):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                        str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation analysis\n",
    "if records and len(records) > 0:\n",
    "    print(\"Cross-tabulation Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nüåä Wave Type vs Direction:\")\n",
    "    crosstab = pd.crosstab(df['wave_type'], df['direction'])\n",
    "    print(crosstab)\n",
    "    \n",
    "    # Visualize cross-tabulation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "    plt.title('Wave Type vs Direction Cross-tabulation')\n",
    "    plt.xlabel('Direction')\n",
    "    plt.ylabel('Wave Type')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Height by wave type analysis\n",
    "    print(\"\\nüìä Height statistics by wave type:\")\n",
    "    height_by_type = df.groupby('wave_type')['height_meters'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "    print(height_by_type.round(3))\n",
    "else:\n",
    "    print(\"No data available for cross-tabulation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height distribution by wave type\n",
    "if records and len(records) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df.boxplot(column='height_meters', by='wave_type', ax=plt.gca())\n",
    "    plt.title('Wave Height Distribution by Wave Type')\n",
    "    plt.suptitle('')  # Remove default title\n",
    "    plt.xlabel('Wave Type')\n",
    "    plt.ylabel('Height (meters)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Violin plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    wave_types = df['wave_type'].unique()\n",
    "    heights_by_type = [df[df['wave_type'] == wt]['height_meters'].values for wt in wave_types]\n",
    "    \n",
    "    parts = plt.violinplot(heights_by_type, positions=range(len(wave_types)), showmeans=True)\n",
    "    plt.xticks(range(len(wave_types)), wave_types, rotation=45)\n",
    "    plt.xlabel('Wave Type')\n",
    "    plt.ylabel('Height (meters)')\n",
    "    plt.title('Height Distribution Density by Wave Type')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for height distribution analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "if records and len(records) > 0:\n",
    "    print(\"Data Quality Checks:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"‚úì Total records: {len(df)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"‚úì No missing values found\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing values found:\")\n",
    "        for col, count in missing_values[missing_values > 0].items():\n",
    "            print(f\"  {col}: {count}\")\n",
    "    \n",
    "    # Check height range\n",
    "    min_height, max_height = df['height_meters'].min(), df['height_meters'].max()\n",
    "    print(f\"‚úì Height range: {min_height:.2f}m to {max_height:.2f}m\")\n",
    "    \n",
    "    if min_height < 0:\n",
    "        print(\"‚ö†Ô∏è Warning: Negative height values found\")\n",
    "        negative_heights = df[df['height_meters'] < 0]\n",
    "        print(f\"  {len(negative_heights)} negative values\")\n",
    "    \n",
    "    if max_height > 10:\n",
    "        print(\"‚ö†Ô∏è Warning: Very large height values found (>10m)\")\n",
    "        large_heights = df[df['height_meters'] > 10]\n",
    "        print(f\"  {len(large_heights)} values > 10m\")\n",
    "    \n",
    "    # Check for valid wave types and directions\n",
    "    valid_wave_types = {\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"}\n",
    "    valid_directions = {\"left\", \"right\", \"both\"}\n",
    "    \n",
    "    actual_wave_types = set(df['wave_type'])\n",
    "    actual_directions = set(df['direction'])\n",
    "    \n",
    "    invalid_wave_types = actual_wave_types - valid_wave_types\n",
    "    invalid_directions = actual_directions - valid_directions\n",
    "    \n",
    "    if not invalid_wave_types:\n",
    "        print(\"‚úì All wave types are valid\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid wave types found: {invalid_wave_types}\")\n",
    "    \n",
    "    if not invalid_directions:\n",
    "        print(\"‚úì All directions are valid\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid directions found: {invalid_directions}\")\n",
    "    \n",
    "    # Check for duplicate image paths\n",
    "    duplicate_paths = df[df.duplicated('image_path', keep=False)]\n",
    "    if len(duplicate_paths) == 0:\n",
    "        print(\"‚úì No duplicate image paths found\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {len(duplicate_paths)} duplicate image paths found\")\n",
    "    \n",
    "    # Check confidence levels\n",
    "    valid_confidences = {\"high\", \"medium\", \"low\"}\n",
    "    actual_confidences = set(df['confidence'])\n",
    "    invalid_confidences = actual_confidences - valid_confidences\n",
    "    \n",
    "    if not invalid_confidences:\n",
    "        print(\"‚úì All confidence levels are valid\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid confidence levels found: {invalid_confidences}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Index file saved to: {CONFIG['out_index']}\")\n",
    "else:\n",
    "    print(\"No records available for quality checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Records Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample records\n",
    "if records and len(records) > 0:\n",
    "    print(\"Sample records from the dataset:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show first 10 records in a nice format\n",
    "    sample_df = df.head(10).copy()\n",
    "    \n",
    "    # Truncate long paths for display\n",
    "    sample_df['image_path'] = sample_df['image_path'].apply(\n",
    "        lambda x: '...' + x[-30:] if len(x) > 33 else x\n",
    "    )\n",
    "    \n",
    "    # Select key columns for display\n",
    "    display_cols = ['image_path', 'height_meters', 'wave_type', 'direction', 'confidence']\n",
    "    print(sample_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    if len(df) > 10:\n",
    "        print(f\"\\n... and {len(df) - 10} more records\")\n",
    "else:\n",
    "    print(\"No records to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics\n",
    "if records and len(records) > 0:\n",
    "    summary_stats = {\n",
    "        \"dataset_info\": {\n",
    "            \"total_samples\": len(df),\n",
    "            \"created_from\": \"real_data\" if os.path.exists(CONFIG[\"labels_json\"]) else \"dummy_data\",\n",
    "            \"source_files\": {\n",
    "                \"labels_json\": CONFIG[\"labels_json\"],\n",
    "                \"images_dir\": CONFIG[\"images_dir\"],\n",
    "                \"output_index\": CONFIG[\"out_index\"]\n",
    "            }\n",
    "        },\n",
    "        \"height_stats\": df['height_meters'].describe().to_dict(),\n",
    "        \"wave_type_counts\": df['wave_type'].value_counts().to_dict(),\n",
    "        \"direction_counts\": df['direction'].value_counts().to_dict(),\n",
    "        \"confidence_counts\": df['confidence'].value_counts().to_dict(),\n",
    "        \"cross_tabulation\": pd.crosstab(df['wave_type'], df['direction']).to_dict(),\n",
    "        \"height_by_wave_type\": df.groupby('wave_type')['height_meters'].agg(['mean', 'std', 'count']).to_dict()\n",
    "    }\n",
    "    \n",
    "    summary_path = CONFIG[\"out_index\"].replace('.jsonl', '_summary.json')\n",
    "    save_json(summary_stats, summary_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Summary statistics saved to: {summary_path}\")\n",
    "    \n",
    "    # Also save a simple CSV for easy viewing\n",
    "    csv_path = CONFIG[\"out_index\"].replace('.jsonl', '_summary.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úÖ Dataset CSV saved to: {csv_path}\")\n",
    "    \n",
    "    print(f\"\\nüìä Final Summary:\")\n",
    "    print(f\"  Total samples: {len(df)}\")\n",
    "    print(f\"  Height range: {df['height_meters'].min():.2f}m - {df['height_meters'].max():.2f}m\")\n",
    "    print(f\"  Wave types: {len(df['wave_type'].unique())}\")\n",
    "    print(f\"  Directions: {len(df['direction'].unique())}\")\n",
    "    print(f\"  High confidence samples: {(df['confidence'] == 'high').sum()}\")\n",
    "else:\n",
    "    print(\"No data available to save summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete, standalone system for building real dataset indices:\n",
    "\n",
    "### ‚úÖ **What we accomplished:**\n",
    "1. **Flexible Data Loading**: Handles both real and dummy data seamlessly\n",
    "2. **Index Creation**: Converts JSON labels to JSONL format for efficient processing\n",
    "3. **Data Validation**: Comprehensive quality checks and error detection\n",
    "4. **Statistical Analysis**: Detailed dataset statistics and distributions\n",
    "5. **Visualization**: Rich plots showing data characteristics\n",
    "6. **Export Capabilities**: Multiple output formats (JSONL, JSON, CSV)\n",
    "\n",
    "### üîß **Key Features:**\n",
    "- **Dummy Data Generation**: Creates realistic sample data when real data is unavailable\n",
    "- **Robust Error Handling**: Graceful handling of missing files and invalid data\n",
    "- **Comprehensive Validation**: Checks for data quality issues and inconsistencies\n",
    "- **Rich Visualizations**: Distribution plots, cross-tabulations, and statistical summaries\n",
    "- **Multiple Export Formats**: JSONL for processing, JSON for metadata, CSV for viewing\n",
    "\n",
    "### üìä **Data Processing:**\n",
    "- **Label Validation**: Ensures all required fields are present and valid\n",
    "- **Image Verification**: Checks for corresponding image files\n",
    "- **Statistical Analysis**: Height distributions, class balance, confidence levels\n",
    "- **Cross-tabulation**: Relationships between wave types and directions\n",
    "\n",
    "### üöÄ **Usage:**\n",
    "- **With Real Data**: Point to your `labels.json` and `images/` directory\n",
    "- **Without Real Data**: Automatically creates dummy data for demonstration\n",
    "- **Quality Assurance**: Run validation checks on your dataset\n",
    "- **Data Exploration**: Understand your dataset characteristics before training\n",
    "\n",
    "### üìÅ **Output Files:**\n",
    "- `real_index.jsonl`: Main dataset index for training\n",
    "- `real_index_summary.json`: Detailed statistics and metadata\n",
    "- `real_index_summary.csv`: Human-readable dataset summary\n",
    "- `labels.json`: Created if using dummy data\n",
    "\n",
    "**This notebook is completely standalone and works with or without real data files!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}