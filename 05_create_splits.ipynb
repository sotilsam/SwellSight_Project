{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train/Validation/Test Splits\n",
    "\n",
    "This notebook creates stratified train/validation/test splits from the real dataset index.\n",
    "\n",
    "## Purpose\n",
    "- Load real dataset index\n",
    "- Perform stratified splitting by wave type and direction\n",
    "- Ensure balanced representation across splits\n",
    "- Analyze split statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions from previous notebooks\n",
    "%run 02_data_loading.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REAL_INDEX = \"data/processed/real_index.jsonl\"\n",
    "OUT_DIR = \"data/processed/splits\"\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1  # Remaining after train and val\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Input index: {REAL_INDEX}\")\n",
    "print(f\"Output directory: {OUT_DIR}\")\n",
    "print(f\"Split ratios - Train: {TRAIN_RATIO}, Val: {VAL_RATIO}, Test: {TEST_RATIO}\")\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Splitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(items, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
    "    \"\"\"Perform stratified split by wave_type and direction.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # Group by wave_type and direction\n",
    "    buckets = defaultdict(list)\n",
    "    for r in items:\n",
    "        key = (r[\"wave_type\"], r[\"direction\"])\n",
    "        buckets[key].append(r)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "\n",
    "    print(f\"Stratifying by {len(buckets)} unique (wave_type, direction) combinations:\")\n",
    "    \n",
    "    for key, bucket in buckets.items():\n",
    "        rng.shuffle(bucket)\n",
    "        n = len(bucket)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        n_train = int(round(n * train_ratio))\n",
    "        n_val = int(round(n * val_ratio))\n",
    "        \n",
    "        # Ensure we don't exceed bucket size\n",
    "        n_train = min(n_train, n)\n",
    "        n_val = min(n_val, n - n_train)\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        print(f\"  {key}: {n} total → {n_train} train, {n_val} val, {n_test} test\")\n",
    "        \n",
    "        train.extend(bucket[:n_train])\n",
    "        val.extend(bucket[n_train:n_train + n_val])\n",
    "        test.extend(bucket[n_train + n_val:])\n",
    "\n",
    "    # Final shuffle\n",
    "    rng.shuffle(train)\n",
    "    rng.shuffle(val)\n",
    "    rng.shuffle(test)\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real dataset index\n",
    "try:\n",
    "    items = read_jsonl(REAL_INDEX)\n",
    "    print(f\"✓ Loaded {len(items)} items from {REAL_INDEX}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ Index file not found: {REAL_INDEX}\")\n",
    "    print(\"Please run the 04_build_real_index.ipynb notebook first.\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "    items = [\n",
    "        {\n",
    "            \"image_path\": f\"data/real/images/img_{i:03d}.jpg\",\n",
    "            \"height_meters\": float(np.random.uniform(0.5, 2.5)),\n",
    "            \"wave_type\": np.random.choice([\"beach_break\", \"reef_break\", \"point_break\", \"closeout\", \"a_frame\"]),\n",
    "            \"direction\": np.random.choice([\"left\", \"right\", \"both\"]),\n",
    "            \"confidence\": np.random.choice([\"high\", \"medium\", \"low\"]),\n",
    "            \"notes\": f\"Example note {i}\",\n",
    "            \"data_key\": i,\n",
    "            \"source\": \"real\"\n",
    "        }\n",
    "        for i in range(100)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data distribution before splitting\n",
    "df_full = pd.DataFrame(items)\n",
    "\n",
    "print(\"Dataset overview before splitting:\")\n",
    "print(f\"Total samples: {len(df_full)}\")\n",
    "print(f\"\\nWave type distribution:\")\n",
    "print(df_full['wave_type'].value_counts())\n",
    "print(f\"\\nDirection distribution:\")\n",
    "print(df_full['direction'].value_counts())\n",
    "\n",
    "# Show stratification keys\n",
    "strat_keys = df_full.groupby(['wave_type', 'direction']).size()\n",
    "print(f\"\\nStratification groups (wave_type, direction):\")\n",
    "print(strat_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed and perform stratified split\n",
    "set_seed(SEED)\n",
    "\n",
    "train, val, test = stratified_split(\n",
    "    items, \n",
    "    train_ratio=TRAIN_RATIO, \n",
    "    val_ratio=VAL_RATIO, \n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit results:\")\n",
    "print(f\"Train: {len(train)} samples ({len(train)/len(items)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val)} samples ({len(val)/len(items)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test)} samples ({len(test)/len(items)*100:.1f}%)\")\n",
    "print(f\"Total: {len(train) + len(val) + len(test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits to JSONL files\n",
    "ensure_dir(OUT_DIR)\n",
    "\n",
    "train_path = f\"{OUT_DIR}/train.jsonl\"\n",
    "val_path = f\"{OUT_DIR}/val.jsonl\"\n",
    "test_path = f\"{OUT_DIR}/test.jsonl\"\n",
    "\n",
    "write_jsonl(train, train_path)\n",
    "write_jsonl(val, val_path)\n",
    "write_jsonl(test, test_path)\n",
    "\n",
    "print(f\"✓ Saved splits:\")\n",
    "print(f\"  Train: {train_path}\")\n",
    "print(f\"  Val: {val_path}\")\n",
    "print(f\"  Test: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert splits to DataFrames for analysis\n",
    "df_train = pd.DataFrame(train)\n",
    "df_val = pd.DataFrame(val)\n",
    "df_test = pd.DataFrame(test)\n",
    "\n",
    "# Add split labels for combined analysis\n",
    "df_train['split'] = 'train'\n",
    "df_val['split'] = 'val'\n",
    "df_test['split'] = 'test'\n",
    "\n",
    "df_combined = pd.concat([df_train, df_val, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution preservation\n",
    "print(\"Distribution analysis across splits:\")\n",
    "print(\"\\n1. Wave Type Distribution:\")\n",
    "wave_type_dist = pd.crosstab(df_combined['wave_type'], df_combined['split'], normalize='index') * 100\n",
    "print(wave_type_dist.round(1))\n",
    "\n",
    "print(\"\\n2. Direction Distribution:\")\n",
    "direction_dist = pd.crosstab(df_combined['direction'], df_combined['split'], normalize='index') * 100\n",
    "print(direction_dist.round(1))\n",
    "\n",
    "print(\"\\n3. Combined (Wave Type, Direction) Distribution:\")\n",
    "df_combined['strat_key'] = df_combined['wave_type'] + '_' + df_combined['direction']\n",
    "combined_dist = pd.crosstab(df_combined['strat_key'], df_combined['split'], normalize='index') * 100\n",
    "print(combined_dist.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize split distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Height distributions by split\n",
    "for i, (split_name, split_df) in enumerate([(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]):\n",
    "    axes[0, i].hist(split_df['height_meters'], bins=15, alpha=0.7, edgecolor='black')\n",
    "    axes[0, i].set_title(f'{split_name} - Height Distribution')\n",
    "    axes[0, i].set_xlabel('Height (meters)')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "    axes[0, i].axvline(split_df['height_meters'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {split_df[\"height_meters\"].mean():.2f}m')\n",
    "    axes[0, i].legend()\n",
    "\n",
    "# Wave type distributions\n",
    "wave_type_counts = [df_train['wave_type'].value_counts(), \n",
    "                   df_val['wave_type'].value_counts(), \n",
    "                   df_test['wave_type'].value_counts()]\n",
    "\n",
    "for i, (split_name, counts) in enumerate(zip([\"Train\", \"Val\", \"Test\"], wave_type_counts)):\n",
    "    axes[1, i].bar(counts.index, counts.values)\n",
    "    axes[1, i].set_title(f'{split_name} - Wave Type Distribution')\n",
    "    axes[1, i].set_xlabel('Wave Type')\n",
    "    axes[1, i].set_ylabel('Count')\n",
    "    axes[1, i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for distribution similarity\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "\n",
    "print(\"Statistical tests for split similarity:\")\n",
    "\n",
    "# Kolmogorov-Smirnov test for height distributions\n",
    "ks_train_val = ks_2samp(df_train['height_meters'], df_val['height_meters'])\n",
    "ks_train_test = ks_2samp(df_train['height_meters'], df_test['height_meters'])\n",
    "ks_val_test = ks_2samp(df_val['height_meters'], df_test['height_meters'])\n",
    "\n",
    "print(f\"\\nHeight distribution similarity (KS test p-values):\")\n",
    "print(f\"Train vs Val: {ks_train_val.pvalue:.4f}\")\n",
    "print(f\"Train vs Test: {ks_train_test.pvalue:.4f}\")\n",
    "print(f\"Val vs Test: {ks_val_test.pvalue:.4f}\")\n",
    "print(\"(Higher p-values indicate more similar distributions)\")\n",
    "\n",
    "# Chi-square test for categorical distributions\n",
    "wave_type_contingency = pd.crosstab(df_combined['wave_type'], df_combined['split'])\n",
    "chi2_wt, p_wt, _, _ = chi2_contingency(wave_type_contingency)\n",
    "\n",
    "direction_contingency = pd.crosstab(df_combined['direction'], df_combined['split'])\n",
    "chi2_dir, p_dir, _, _ = chi2_contingency(direction_contingency)\n",
    "\n",
    "print(f\"\\nCategorical distribution similarity (Chi-square test p-values):\")\n",
    "print(f\"Wave type across splits: {p_wt:.4f}\")\n",
    "print(f\"Direction across splits: {p_dir:.4f}\")\n",
    "print(\"(Higher p-values indicate more similar distributions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed split summary\n",
    "print(\"Detailed Split Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for split_name, split_df in [(\"TRAIN\", df_train), (\"VALIDATION\", df_val), (\"TEST\", df_test)]:\n",
    "    print(f\"\\n{split_name} SET:\")\n",
    "    print(f\"  Samples: {len(split_df)}\")\n",
    "    print(f\"  Height range: {split_df['height_meters'].min():.2f}m - {split_df['height_meters'].max():.2f}m\")\n",
    "    print(f\"  Height mean: {split_df['height_meters'].mean():.2f}m ± {split_df['height_meters'].std():.2f}m\")\n",
    "    \n",
    "    print(f\"  Wave types: {dict(split_df['wave_type'].value_counts())}\")\n",
    "    print(f\"  Directions: {dict(split_df['direction'].value_counts())}\")\n",
    "    print(f\"  Confidence levels: {dict(split_df['confidence'].value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "print(\"Split Validation Checks:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check no overlap between splits\n",
    "train_paths = set(r['image_path'] for r in train)\n",
    "val_paths = set(r['image_path'] for r in val)\n",
    "test_paths = set(r['image_path'] for r in test)\n",
    "\n",
    "overlap_train_val = train_paths & val_paths\n",
    "overlap_train_test = train_paths & test_paths\n",
    "overlap_val_test = val_paths & test_paths\n",
    "\n",
    "if not overlap_train_val and not overlap_train_test and not overlap_val_test:\n",
    "    print(\"✓ No overlap between splits\")\n",
    "else:\n",
    "    print(\"✗ Overlap detected between splits!\")\n",
    "    if overlap_train_val:\n",
    "        print(f\"  Train-Val overlap: {len(overlap_train_val)} samples\")\n",
    "    if overlap_train_test:\n",
    "        print(f\"  Train-Test overlap: {len(overlap_train_test)} samples\")\n",
    "    if overlap_val_test:\n",
    "        print(f\"  Val-Test overlap: {len(overlap_val_test)} samples\")\n",
    "\n",
    "# Check total count\n",
    "total_split = len(train) + len(val) + len(test)\n",
    "if total_split == len(items):\n",
    "    print(f\"✓ All {len(items)} samples accounted for in splits\")\n",
    "else:\n",
    "    print(f\"✗ Sample count mismatch: {total_split} in splits vs {len(items)} original\")\n",
    "\n",
    "# Check minimum samples per class in each split\n",
    "min_samples_per_class = 1\n",
    "for split_name, split_df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    wt_counts = split_df['wave_type'].value_counts()\n",
    "    dir_counts = split_df['direction'].value_counts()\n",
    "    \n",
    "    if wt_counts.min() >= min_samples_per_class and dir_counts.min() >= min_samples_per_class:\n",
    "        print(f\"✓ {split_name.capitalize()} split has sufficient samples per class\")\n",
    "    else:\n",
    "        print(f\"⚠️ {split_name.capitalize()} split has classes with few samples:\")\n",
    "        if wt_counts.min() < min_samples_per_class:\n",
    "            print(f\"  Wave types: {wt_counts[wt_counts < min_samples_per_class].to_dict()}\")\n",
    "        if dir_counts.min() < min_samples_per_class:\n",
    "            print(f\"  Directions: {dir_counts[dir_counts < min_samples_per_class].to_dict()}\")\n",
    "\n",
    "print(f\"\\n✓ Splits saved successfully to {OUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Split Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save split statistics\n",
    "split_stats = {\n",
    "    \"total_samples\": len(items),\n",
    "    \"split_sizes\": {\n",
    "        \"train\": len(train),\n",
    "        \"val\": len(val),\n",
    "        \"test\": len(test)\n",
    "    },\n",
    "    \"split_ratios\": {\n",
    "        \"train\": len(train) / len(items),\n",
    "        \"val\": len(val) / len(items),\n",
    "        \"test\": len(test) / len(items)\n",
    "    },\n",
    "    \"height_stats\": {\n",
    "        \"train\": df_train['height_meters'].describe().to_dict(),\n",
    "        \"val\": df_val['height_meters'].describe().to_dict(),\n",
    "        \"test\": df_test['height_meters'].describe().to_dict()\n",
    "    },\n",
    "    \"wave_type_distribution\": {\n",
    "        \"train\": df_train['wave_type'].value_counts().to_dict(),\n",
    "        \"val\": df_val['wave_type'].value_counts().to_dict(),\n",
    "        \"test\": df_test['wave_type'].value_counts().to_dict()\n",
    "    },\n",
    "    \"direction_distribution\": {\n",
    "        \"train\": df_train['direction'].value_counts().to_dict(),\n",
    "        \"val\": df_val['direction'].value_counts().to_dict(),\n",
    "        \"test\": df_test['direction'].value_counts().to_dict()\n",
    "    },\n",
    "    \"stratification_groups\": len(strat_keys),\n",
    "    \"seed\": SEED\n",
    "}\n",
    "\n",
    "stats_path = f\"{OUT_DIR}/split_statistics.json\"\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(split_stats, f, indent=2)\n",
    "\n",
    "print(f\"Split statistics saved to: {stats_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}